<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Unitree G1 EDU-U6: Kompendium InÅ¼ynierskie</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@300;400;500;600;700;800&family=JetBrains+Mono:wght@400;500&display=swap');

        :root {
            --accent: #f59e0b;
            --accent-dark: #d97706;
            --bg-main: #fcfcfb;
        }

        body {
            font-family: 'Plus Jakarta Sans', sans-serif;
            background-color: var(--bg-main);
            overflow: hidden;
            margin: 0;
        }

        .code-container::-webkit-scrollbar { height: 6px; width: 6px; }
        code, pre { font-family: 'JetBrains Mono', monospace; }

        #main-scroll-area {
            height: calc(100vh - 80px);
            overflow-y: auto;
            scroll-behavior: smooth;
        }

        .pro-tip    { border-left: 4px solid #3b82f6; background: #eff6ff; }
        .critical-warn { border-left: 4px solid #ef4444; background: #fef2f2; }
        .info-box   { border-left: 4px solid #10b981; background: #f0fdf4; }

        .code-block {
            background: #1c1917;
            border-radius: 0.75rem;
            padding: 1.25rem;
            color: #f59e0b;
            font-size: 0.875rem;
            overflow-x: auto;
            margin-bottom: 1.5rem;
        }

        .nav-item-active {
            background: #f59e0b;
            color: #0c0a09 !important;
            font-weight: 700;
        }

        .custom-scrollbar::-webkit-scrollbar { width: 4px; }
        .custom-scrollbar::-webkit-scrollbar-track { background: transparent; }
        .custom-scrollbar::-webkit-scrollbar-thumb { background: #444; border-radius: 10px; }

        .step-content { animation: fadeIn 0.8s ease-out; }

        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to   { opacity: 1; transform: translateY(0); }
        }

        .section-divider {
            border: none;
            border-top: 2px dashed #e7e5e4;
            margin: 2.5rem 0;
        }

        .section-label {
            display: inline-flex;
            align-items: center;
            gap: 0.5rem;
            background: #f59e0b;
            color: #0c0a09;
            font-size: 0.65rem;
            font-weight: 800;
            letter-spacing: 0.1em;
            text-transform: uppercase;
            padding: 0.25rem 0.75rem;
            border-radius: 999px;
            margin-bottom: 1rem;
        }

        .nvidia-badge {
            display: inline-block;
            background: #76b900;
            color: #000;
            font-size: 0.6rem;
            font-weight: 900;
            letter-spacing: 0.08em;
            text-transform: uppercase;
            padding: 0.2rem 0.6rem;
            border-radius: 999px;
            margin-bottom: 0.75rem;
        }
    </style>
</head>
<body class="text-stone-900">

    <div class="fixed top-0 left-0 w-full h-1 bg-stone-200 z-[100]">
        <div id="top-progress" class="h-full bg-amber-500 transition-all duration-500" style="width: 0%"></div>
    </div>

    <div class="flex h-screen overflow-hidden">

        <!-- Sidebar -->
        <aside id="sidebar" class="fixed inset-y-0 left-0 w-80 bg-stone-950 text-stone-400 transform -translate-x-full lg:translate-x-0 transition-transform duration-300 z-50 flex flex-col border-r border-stone-800">
            <div class="p-8 border-b border-stone-800">
                <div class="flex items-center gap-3 mb-2">
                    <h1 class="text-white font-extrabold text-3xl tracking-tight leading-none">Robo<br></h1>
                    <div class="w-14 h-8 bg-amber-500 rounded-lg flex items-center justify-center text-stone-950 font-black text-2xl">hub</div>
                </div>
                <div><span class="text-amber-500 text-xs font-medium">Unitree Deployment Portal</span></div>
            </div>

            <nav id="nav-list" class="flex-1 overflow-y-auto p-4 space-y-1 custom-scrollbar">
                <!-- Injected via JS -->
            </nav>

            <div class="p-6 bg-stone-900/50 border-t text-medium border-stone-800">
                <div class="flex justify-between text-[10px] font-bold uppercase mb-2">
                    <span>GotowoÅ›Ä‡ systemu</span>
                    <span id="progress-val">0%</span>
                </div>
                <div class="h-1 w-full bg-stone-800 rounded-full overflow-hidden">
                    <div id="inner-progress" class="h-full bg-amber-500 transition-all duration-500" style="width: 0%"></div>
                </div>
            </div>
        </aside>

        <!-- Main Content -->
        <main class="flex-1 lg:ml-80 flex flex-col h-screen overflow-hidden">

            <button onclick="toggleSidebar()" class="lg:hidden absolute top-4 left-4 z-[60] p-2 bg-white rounded-lg shadow-md border border-stone-200">
                <span class="text-2xl">â˜°</span>
            </button>

            <div id="main-scroll-area">
                <div id="content-mount" class="p-6 lg:p-16 max-w-4xl mx-auto w-full">
                    <!-- Section Content Injected by JS -->
                </div>
            </div>

            <!-- Sticky Footer -->
            <footer class="h-20 bg-white/90 backdrop-blur-md border-t border-stone-200 px-6 flex items-center justify-between z-40">
                <button id="prev-btn" onclick="changeModule(-1)" class="px-6 py-2 text-stone-400 font-bold hover:text-amber-600 transition-all disabled:opacity-0">
                    â† PowrÃ³t
                </button>
                <div id="footer-label" class="hidden md:block text-[10px] font-bold text-stone-400 uppercase tracking-widest">
                    Status: Konfigurowanie
                </div>
                <button id="next-btn" onclick="changeModule(1)" class="px-8 py-3 bg-stone-900 text-white font-bold rounded-xl hover:bg-amber-600 transition-all shadow-lg active:scale-95">
                    NastÄ™pny Krok â†’
                </button>
            </footer>
        </main>
    </div>

    <script>
        const modules = [

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 1: PRZYGOTOWANIE SYSTEMU
               (poÅ‚Ä…czenie: Ubuntu 22.04 + Konfiguracja OS + SieÄ‡ & CycloneDDS)
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'setup',
                title: 'Przygotowanie Systemu',
                icon: 'ğŸ§',
                links: [
                    {name: 'Ubuntu 22.04', url: 'https://releases.ubuntu.com/jammy/'},
                    {name: 'NVIDIA Drivers', url: 'https://www.nvidia.com/en-us/drivers/'},
                    {name: 'CycloneDDS', url: 'https://github.com/eclipse-cyclonedds/cyclonedds'}
                ],
                description: 'Kompletna konfiguracja Ubuntu 22.04 dla Unitree G1 EDU â€“ od instalacji jÄ…dra niskolatencyjnego, przez sterowniki NVIDIA, po strojenie warstwy sieciowej DDS.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- CZÄ˜ÅšÄ† 1: Ubuntu i narzÄ™dzia -->
                        <div class="section-label">ğŸ§ CzÄ™Å›Ä‡ 1 â€” Ubuntu 22.04 &amp; NarzÄ™dzia bazowe</div>

                        <p class="text-stone-600">Unitree G1 komunikuje siÄ™ przez protokÃ³Å‚ UDP. Aby zapewniÄ‡ stabilnoÅ›Ä‡, system musi byÄ‡ zoptymalizowany pod kÄ…tem opÃ³ÅºnieÅ„ sieciowych oraz obsÅ‚ugi kart NVIDIA wymaganych przez Isaac Sim.</p>

                        <div class="bg-amber-50 border-l-4 border-amber-500 p-4 rounded-r-xl">
                            <h4 class="font-bold text-amber-800 mb-2">WaÅ¼ne przed startem</h4>
                            <ul class="text-sm text-amber-700 list-disc ml-5 space-y-1">
                                <li>Zainstaluj Ubuntu 22.04 w wersji Desktop (zalecane jÄ…dro 5.15+ LTS).</li>
                                <li>Nie instaluj Wayland â€“ uÅ¼ywaj Xorg dla peÅ‚nej kompatybilnoÅ›ci z NVIDIA.</li>
                                <li>OdÅ‚Ä…cz poÅ‚Ä…czenie Wi-Fi podczas konfiguracji interfejsu Ethernet z robotem.</li>
                            </ul>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">1. Aktualizacja i podstawowe narzÄ™dzia</h3>
                            <div class="code-block">
                                <pre><code>sudo apt update && sudo apt upgrade -y
sudo apt install build-essential cmake git curl python3-pip -y</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">2. Sterowniki NVIDIA (wymagane przez Isaac Sim)</h3>
                            <p class="text-stone-500 text-sm italic mb-3">Wymagane do symulacji fizycznej i renderowania RTX.</p>
                            <div class="code-block">
                                <pre><code>sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
sudo apt install nvidia-driver-535 -y
# Zrestartuj system, nastÄ™pnie sprawdÅº: nvidia-smi</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- CZÄ˜ÅšÄ† 2: Konfiguracja OS -->
                        <div class="section-label">âš™ï¸ CzÄ™Å›Ä‡ 2 â€” Konfiguracja OS pod niskie opÃ³Åºnienia</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">3. JÄ…dro lowlatency</h3>
                            <p class="text-stone-600 mb-3">Robot G1 przesyÅ‚a tysiÄ…ce pakietÃ³w na sekundÄ™. Standardowe jÄ…dro Ubuntu moÅ¼e powodowaÄ‡ â€jitter". Zaleca siÄ™ instalacjÄ™ jÄ…dra <strong>lowlatency</strong>.</p>
                            <div class="code-block">
                                <pre><code># Instalacja jÄ…dra lowlatency
sudo apt install linux-image-lowlatency linux-headers-lowlatency -y
# Po restarcie sprawdÅº: uname -a</code></pre>
                            </div>
                        </section>

                        <div class="critical-warn p-5 rounded-xl">
                            <h4 class="font-bold text-red-800">Ustawienia NVIDIA PowerMizer</h4>
                            <p class="text-sm text-red-700">W panelu NVIDIA X Server Settings ustaw profil PowerMizer na <strong>"Prefer Maximum Performance"</strong>. To eliminuje skoki opÃ³ÅºnieÅ„ przy wnioskowaniu AI.</p>
                        </div>

                        <hr class="section-divider">

                        <!-- CZÄ˜ÅšÄ† 3: SieÄ‡ i CycloneDDS -->
                        <div class="section-label">ğŸ“¶ CzÄ™Å›Ä‡ 3 â€” SieÄ‡, CycloneDDS i zaleÅ¼noÅ›ci SDK</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">4. Konfiguracja interfejsu sieciowego</h3>
                            <p class="text-stone-600 mb-3">G1 komunikuje siÄ™ przez dedykowany port Ethernet. Musisz wymusiÄ‡, aby pakiety DDS nie wychodziÅ‚y przez Wi-Fi.</p>
                            <div class="code-block">
                                <pre><code># SprawdÅº nazwÄ™ interfejsu (np. enp3s0)
ip link show

# UtwÃ³rz plik konfiguracyjny CycloneDDS
cat <<'EOF' > ~/cyclonedds.xml
&lt;CycloneDDS&gt;
  &lt;Domain&gt;
    &lt;General&gt;
      &lt;NetworkInterfaceAddress&gt;enp3s0&lt;/NetworkInterfaceAddress&gt;
      &lt;AllowMulticast&gt;true&lt;/AllowMulticast&gt;
    &lt;/General&gt;
  &lt;/Domain&gt;
&lt;/CycloneDDS&gt;
EOF

# Dodaj do ~/.bashrc dla trwaÅ‚oÅ›ci
echo 'export CYCLONEDDS_URI=file:///home/$USER/cyclonedds.xml' >> ~/.bashrc
source ~/.bashrc</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">5. ZaleÅ¼noÅ›ci systemowe i kompilacja SDK2</h3>
                            <p class="text-stone-600 mb-3">Zanim pobierzemy SDK, przygotujemy system do kompilacji duÅ¼ych projektÃ³w C++ z obsÅ‚ugÄ… CycloneDDS.</p>
                            <div class="code-block">
                                <pre><code>sudo apt install build-essential cmake git libboost-all-dev libpcap-dev -y
# Instalacja CycloneDDS (wymagane przez SDK2 i ROS2)
sudo apt install ros-humble-cyclonedds -y

# Kompilacja Unitree SDK2 z obsÅ‚ugÄ… Pythona
git clone https://github.com/unitreerobotics/unitree_sdk2.git
cd unitree_sdk2 && mkdir build && cd build
cmake .. -DUNITREE_SDK2_PYTHON=ON
make -j$(nproc)
sudo make install</code></pre>
                            </div>
                        </section>

                        <div class="info-box p-5 rounded-xl">
                            <h4 class="font-bold text-green-800">âœ… System gotowy</h4>
                            <p class="text-sm text-green-700">Po wykonaniu powyÅ¼szych krokÃ³w masz: jÄ…dro niskolatencyjne, sterowniki NVIDIA 535, skonfigurowany interfejs DDS i skompilowane biblioteki SDK2. MoÅ¼esz przejÅ›Ä‡ do konfiguracji ROS 2.</p>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 2: ROS 2 & UNITREE SDK2
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'ros2_sdk2',
                title: 'ROS 2 & Unitree SDK2',
                icon: 'ğŸ¤–',
                links: [
                    {name: 'Unitree ROS2 Repo', url: 'https://github.com/unitreerobotics/unitree_ros2'},
                    {name: 'SDK2 Examples', url: 'https://github.com/unitreerobotics/unitree_sdk2'},
                    {name: 'ROS 2 Humble Docs', url: 'https://docs.ros.org/en/humble/index.html'}
                ],
                description: 'PeÅ‚en stos sterowania robotem: architektura wÄ™zÅ‚Ã³w ROS 2, tematy G1, launch files, tf2 oraz niskopoziomowy dostÄ™p do silnikÃ³w przez Unitree SDK2.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ›ï¸ Architektura â€” Jak dziaÅ‚a ROS 2 z G1?</div>

                        <p class="text-stone-600 leading-relaxed">
                            ROS 2 (Robot Operating System 2) to nie system operacyjny, lecz <strong>middleware</strong> â€“ warstwa komunikacyjna Å‚Ä…czÄ…ca niezaleÅ¼ne procesy (wÄ™zÅ‚y) przez ustandaryzowane interfejsy. Unitree G1 EDU integruje siÄ™ z ROS 2 przez mostek DDS, ktÃ³ry przeksztaÅ‚ca wewnÄ™trzne pakiety UDP robota na standardowe wiadomoÅ›ci ROS 2.
                        </p>

                        <!-- Diagram architektury -->
                        <div class="bg-stone-950 rounded-2xl p-6 text-sm font-mono">
                            <p class="text-stone-500 text-[10px] uppercase tracking-widest mb-4">Diagram przepÅ‚ywu danych</p>
                            <div class="space-y-2 text-xs">
                                <div class="flex items-center gap-3">
                                    <span class="px-3 py-1 bg-amber-500 text-stone-900 font-black rounded">G1 Hardware</span>
                                    <span class="text-stone-500">â”€â”€UDP/DDSâ”€â”€â–¶</span>
                                    <span class="px-3 py-1 bg-stone-700 text-amber-400 rounded">unitree_ros2 Bridge</span>
                                    <span class="text-stone-500">â”€â”€Topicsâ”€â”€â–¶</span>
                                    <span class="px-3 py-1 bg-stone-700 text-green-400 rounded">TwÃ³j wÄ™zeÅ‚ ROS 2</span>
                                </div>
                                <div class="flex items-center gap-3 pl-32">
                                    <span class="text-stone-500">â—€â”€â”€Commandsâ”€â”€</span>
                                    <span class="px-3 py-1 bg-stone-700 text-blue-400 rounded">/cmd_vel, /joint_cmd</span>
                                </div>
                            </div>
                        </div>

                        <!-- Kluczowe koncepty -->
                        <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
                            <div class="p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                <span class="block text-amber-500 font-black text-xs uppercase mb-2">ğŸ“¡ Tematy (Topics)</span>
                                <p class="text-xs text-stone-500">StrumieÅ„ danych publish/subscribe. WÄ™zÅ‚y emitujÄ… dane bez wiedzy o odbiorcach. Idealne dla sensorÃ³w i stanÃ³w.</p>
                            </div>
                            <div class="p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                <span class="block text-blue-500 font-black text-xs uppercase mb-2">ğŸ”§ Serwisy (Services)</span>
                                <p class="text-xs text-stone-500">Komunikacja Å¼Ä…danieâ€“odpowiedÅº (request/response). Synchroniczna. Dobra dla jednorazowych komend: "usiÄ…dÅº", "wstaÅ„".</p>
                            </div>
                            <div class="p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                <span class="block text-green-500 font-black text-xs uppercase mb-2">ğŸ¯ Akcje (Actions)</span>
                                <p class="text-xs text-stone-500">DÅ‚ugotrwaÅ‚e zadania z feedbackiem. Np. "przejdÅº 5 metrÃ³w" z raportowaniem postÄ™pu co 100ms.</p>
                            </div>
                        </div>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ“¦ CzÄ™Å›Ä‡ 1 â€” Instalacja i Workspace</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">1. Instalacja ROS 2 Humble</h3>
                            <p class="text-stone-600 mb-3">JeÅ›li ROS 2 nie jest jeszcze zainstalowany, wykonaj poniÅ¼szÄ… sekwencjÄ™. Zalecane na Ubuntu 22.04 LTS.</p>
                            <div class="code-block">
                                <pre><code># Dodaj klucze GPG i repozytorium ROS 2
sudo apt install software-properties-common curl -y
sudo curl -sSL https://raw.githubusercontent.com/ros/rosdistro/master/ros.key \
    -o /usr/share/keyrings/ros-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/ros-archive-keyring.gpg] \
    http://packages.ros.org/ros2/ubuntu $(. /etc/os-release && echo $UBUNTU_CODENAME) main" \
    | sudo tee /etc/apt/sources.list.d/ros2.list > /dev/null

# Instalacja peÅ‚nej wersji desktop (z RViz2, rqt itp.)
sudo apt update
sudo apt install ros-humble-desktop -y
sudo apt install python3-colcon-common-extensions python3-rosdep -y

# Dodaj do ~/.bashrc
echo "source /opt/ros/humble/setup.bash" >> ~/.bashrc
source ~/.bashrc</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">2. Budowanie Workspace Unitree ROS 2</h3>
                            <p class="text-stone-600 mb-3">Pakiet <code>unitree_ros2</code> zawiera definicje wiadomoÅ›ci (msg), interfejsy serwisÃ³w i wÄ™zeÅ‚ mostu DDS dla G1.</p>
                            <div class="code-block">
                                <pre><code># Tworzenie przestrzeni roboczej
mkdir -p ~/unitree_ws/src && cd ~/unitree_ws/src
git clone https://github.com/unitreerobotics/unitree_ros2.git

# Inicjalizacja rosdep i instalacja zaleÅ¼noÅ›ci
cd ~/unitree_ws
sudo rosdep init && rosdep update
rosdep install --from-paths src --ignore-src -r -y

# Kompilacja z optymalizacjÄ…
colcon build --symlink-install --cmake-args -DCMAKE_BUILD_TYPE=Release

# StaÅ‚e ÅºrÃ³dÅ‚owanie workspace (dodaj do ~/.bashrc)
echo "source ~/unitree_ws/install/setup.bash" >> ~/.bashrc
source ~/.bashrc</code></pre>
                            </div>
                        </section>

                        <div class="pro-tip p-5 rounded-xl">
                            <h4 class="font-bold text-blue-800">Weryfikacja instalacji</h4>
                            <p class="text-sm text-blue-700">Po kompilacji uruchom <code>ros2 pkg list | grep unitree</code>. PowinieneÅ› zobaczyÄ‡ pakiety: <code>unitree_go</code>, <code>unitree_hg</code>, <code>unitree_ros2</code>. JeÅ›li robot jest podÅ‚Ä…czony, <code>ros2 topic list</code> pokaÅ¼e aktywne tematy G1.</p>
                        </div>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ“¡ CzÄ™Å›Ä‡ 2 â€” Tematy i Komendy G1</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">3. Mapa kluczowych tematÃ³w ROS 2 dla G1</h3>
                            <p class="text-stone-600 mb-4">PoniÅ¼sza tabela pokazuje najwaÅ¼niejsze tematy dostÄ™pne po uruchomieniu mostu Unitree ROS 2.</p>

                            <div class="overflow-x-auto rounded-xl border border-stone-100">
                                <table class="w-full text-xs border-collapse">
                                    <thead>
                                        <tr class="bg-stone-900 text-amber-400">
                                            <th class="text-left px-4 py-3 font-black">Temat (Topic)</th>
                                            <th class="text-left px-4 py-3 font-black">Typ wiadomoÅ›ci</th>
                                            <th class="text-left px-4 py-3 font-black">Kierunek</th>
                                            <th class="text-left px-4 py-3 font-black">Opis</th>
                                        </tr>
                                    </thead>
                                    <tbody class="divide-y divide-stone-100">
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/cmd_vel</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">geometry_msgs/Twist</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-blue-100 text-blue-700 rounded text-[9px] font-bold">PUB â†’</span></td>
                                            <td class="px-4 py-3 text-stone-600">Komenda prÄ™dkoÅ›ci liniowej i kÄ…towej (chÃ³d)</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/joint_states</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">sensor_msgs/JointState</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-green-100 text-green-700 rounded text-[9px] font-bold">â† SUB</span></td>
                                            <td class="px-4 py-3 text-stone-600">Pozycje, prÄ™dkoÅ›ci i momenty 23+ stawÃ³w</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/imu/data</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">sensor_msgs/Imu</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-green-100 text-green-700 rounded text-[9px] font-bold">â† SUB</span></td>
                                            <td class="px-4 py-3 text-stone-600">Dane IMU: orientacja, przyspieszenie, Å¼yroskop</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/odom</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">nav_msgs/Odometry</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-green-100 text-green-700 rounded text-[9px] font-bold">â† SUB</span></td>
                                            <td class="px-4 py-3 text-stone-600">Estymowana pozycja i orientacja w przestrzeni</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/camera/color/image_raw</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">sensor_msgs/Image</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-green-100 text-green-700 rounded text-[9px] font-bold">â† SUB</span></td>
                                            <td class="px-4 py-3 text-stone-600">Obraz RGB z kamery gÅ‚owowej G1</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/lf/lowstate</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">unitree_hg/LowState</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-green-100 text-green-700 rounded text-[9px] font-bold">â† SUB</span></td>
                                            <td class="px-4 py-3 text-stone-600">PeÅ‚ny stan niskopoziomowy (silniki, IMU, kontakt)</td>
                                        </tr>
                                        <tr class="bg-white hover:bg-stone-50">
                                            <td class="px-4 py-3 font-mono text-blue-600">/lf/lowcmd</td>
                                            <td class="px-4 py-3 font-mono text-stone-500">unitree_hg/LowCmd</td>
                                            <td class="px-4 py-3"><span class="px-2 py-0.5 bg-blue-100 text-blue-700 rounded text-[9px] font-bold">PUB â†’</span></td>
                                            <td class="px-4 py-3 text-stone-600">BezpoÅ›rednie komendy do silnikÃ³w (Kp, Kd, Torque)</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">4. Podstawowe komendy CLI ROS 2</h3>
                            <div class="code-block">
                                <pre><code># Listowanie aktywnych tematÃ³w
ros2 topic list

# PodglÄ…d danych z IMU w czasie rzeczywistym
ros2 topic echo /imu/data

# Sprawdzenie czÄ™stotliwoÅ›ci publikacji tematu
ros2 topic hz /joint_states

# WysÅ‚anie jednorazowej komendy ruchu (x=0.3 m/s do przodu)
ros2 topic pub --once /cmd_vel geometry_msgs/msg/Twist \
  "{linear: {x: 0.3, y: 0.0, z: 0.0}, angular: {x: 0.0, y: 0.0, z: 0.0}}"

# Zapis wszystkich tematÃ³w do pliku bag (rejestracja sesji)
ros2 bag record -a -o ~/session_2024_g1

# Odtworzenie nagranej sesji
ros2 bag play ~/session_2024_g1</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ CzÄ™Å›Ä‡ 3 â€” WÅ‚asne WÄ™zÅ‚y ROS 2 (Python)</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">5. Subskrybent stanu robota (Subscriber)</h3>
                            <p class="text-stone-600 mb-3">Podstawowy wzorzec: wÄ™zeÅ‚ nasÅ‚uchuje na temat <code>/joint_states</code> i loguje pozycje stawÃ³w nÃ³g.</p>
                            <div class="code-block">
                                <pre><code>#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import JointState

class G1StateMonitor(Node):
    def __init__(self):
        super().__init__('g1_state_monitor')
        self.subscription = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_callback,
            10  # QoS â€“ gÅ‚Ä™bokoÅ›Ä‡ kolejki
        )
        self.get_logger().info('Monitor stanu G1 uruchomiony.')

    def joint_callback(self, msg: JointState):
        # WyÅ›wietl nazwy i pozycje pierwszych 6 stawÃ³w (nogi)
        for name, pos in zip(msg.name[:6], msg.position[:6]):
            self.get_logger().info(f'  {name:30s}: {pos:+.4f} rad')

def main():
    rclpy.init()
    node = G1StateMonitor()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">6. Publisher komendy chodu (Publisher)</h3>
                            <p class="text-stone-600 mb-3">WÄ™zeÅ‚, ktÃ³ry przez 5 sekund wysyÅ‚a komendÄ™ chodu do przodu, nastÄ™pnie zatrzymuje robota.</p>
                            <div class="code-block">
                                <pre><code>#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from geometry_msgs.msg import Twist
import time

class G1WalkForward(Node):
    def __init__(self):
        super().__init__('g1_walk_forward')
        self.publisher = self.create_publisher(Twist, '/cmd_vel', 10)
        self.timer = self.create_timer(0.1, self.publish_cmd)  # 10 Hz
        self.start_time = time.time()
        self.duration = 5.0  # sekundy chodu
        self.get_logger().info('Start: Robot idzie do przodu przez 5s...')

    def publish_cmd(self):
        msg = Twist()
        elapsed = time.time() - self.start_time

        if elapsed < self.duration:
            msg.linear.x = 0.3   # 0.3 m/s do przodu
            msg.angular.z = 0.0  # brak obrotu
        else:
            msg.linear.x = 0.0   # STOP
            self.get_logger().info('Zatrzymanie robota.')
            self.timer.cancel()

        self.publisher.publish(msg)

def main():
    rclpy.init()
    node = G1WalkForward()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">7. WÄ™zeÅ‚ Å‚Ä…czony: Odczyt IMU + Logowanie do pliku</h3>
                            <p class="text-stone-600 mb-3">Praktyczny wÄ™zeÅ‚ rejestrujÄ…cy dane z IMU do pliku CSV â€“ przydatny przy analizie stabilnoÅ›ci chodu.</p>
                            <div class="code-block">
                                <pre><code>#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
import csv, math, time

class ImuLogger(Node):
    def __init__(self):
        super().__init__('g1_imu_logger')
        self.sub = self.create_subscription(Imu, '/imu/data', self.imu_cb, 50)
        self.csvfile = open('/tmp/g1_imu_log.csv', 'w', newline='')
        self.writer = csv.writer(self.csvfile)
        self.writer.writerow(['timestamp', 'roll_deg', 'pitch_deg',
                              'accel_x', 'accel_y', 'accel_z'])
        self.get_logger().info('Rejestracja IMU â†’ /tmp/g1_imu_log.csv')

    def imu_cb(self, msg: Imu):
        q = msg.orientation
        # Konwersja quaternion â†’ kÄ…ty Eulera (roll, pitch)
        sinr = 2.0 * (q.w * q.x + q.y * q.z)
        cosr = 1.0 - 2.0 * (q.x**2 + q.y**2)
        roll = math.degrees(math.atan2(sinr, cosr))

        sinp = 2.0 * (q.w * q.y - q.z * q.x)
        pitch = math.degrees(math.asin(max(-1.0, min(1.0, sinp))))

        self.writer.writerow([
            time.time(),
            round(roll, 3), round(pitch, 3),
            msg.linear_acceleration.x,
            msg.linear_acceleration.y,
            msg.linear_acceleration.z
        ])

    def destroy_node(self):
        self.csvfile.close()
        super().destroy_node()

def main():
    rclpy.init()
    node = ImuLogger()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸš€ CzÄ™Å›Ä‡ 4 â€” Launch Files i Parametry</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">8. Plik Launch â€“ uruchamianie wielu wÄ™zÅ‚Ã³w jednoczeÅ›nie</h3>
                            <p class="text-stone-600 mb-3">Launch files pozwalajÄ… jednÄ… komendÄ… uruchomiÄ‡ most DDS, monitor stanu i wizualizacjÄ™ w RViz2.</p>
                            <div class="code-block">
                                <pre><code># Plik: ~/unitree_ws/src/my_g1_pkg/launch/g1_full.launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
from launch.actions import DeclareLaunchArgument
from launch.substitutions import LaunchConfiguration

def generate_launch_description():
    return LaunchDescription([
        # Argument: interfejs sieciowy (domyÅ›lnie eth0)
        DeclareLaunchArgument('interface', default_value='eth0'),

        # Most DDS â†” ROS 2
        Node(
            package='unitree_ros2',
            executable='unitree_ros2_bridge',
            name='g1_bridge',
            parameters=[{'network_interface': LaunchConfiguration('interface')}],
            output='screen'
        ),

        # Monitor stanu (nasz wÄ™zeÅ‚)
        Node(
            package='my_g1_pkg',
            executable='state_monitor',
            name='g1_state_monitor',
            output='screen'
        ),

        # RViz2 z predefiniowanÄ… konfiguracjÄ…
        Node(
            package='rviz2',
            executable='rviz2',
            name='rviz2',
            arguments=['-d', '/path/to/g1_config.rviz'],
        ),
    ])</code></pre>
                            </div>

                            <div class="code-block">
                                <pre><code># Uruchomienie z domyÅ›lnym interfejsem
ros2 launch my_g1_pkg g1_full.launch.py

# Uruchomienie z niestandardowym interfejsem sieciowym
ros2 launch my_g1_pkg g1_full.launch.py interface:=enp3s0</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ—ºï¸ CzÄ™Å›Ä‡ 5 â€” tf2: System Transformacji</div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">9. Drzewo transformacji G1 (tf2)</h3>
                            <p class="text-stone-600 mb-3">
                                <strong>tf2</strong> to biblioteka Å›ledzÄ…ca wzajemne poÅ‚oÅ¼enie wszystkich ukÅ‚adÃ³w odniesienia robota w czasie. Pozwala zapytaÄ‡: â€gdzie jest chwytaĞº prawej rÄ™ki wzglÄ™dem kamery?"
                            </p>
                            <div class="code-block">
                                <pre><code># PodglÄ…d aktualnego drzewa transformacji
ros2 run tf2_tools view_frames

# Przelicz pozycjÄ™ punktu z ukÅ‚adu kamery do ukÅ‚adu Å›wiata
ros2 run tf2_ros tf2_echo world camera_link

# Python: pobranie transformacji miÄ™dzy ukÅ‚adami
import rclpy
from rclpy.node import Node
from tf2_ros import Buffer, TransformListener

class TfReader(Node):
    def __init__(self):
        super().__init__('tf_reader')
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        self.timer = self.create_timer(1.0, self.get_transform)

    def get_transform(self):
        try:
            t = self.tf_buffer.lookup_transform(
                'base_link',        # ukÅ‚ad docelowy
                'right_hand_link',  # ukÅ‚ad ÅºrÃ³dÅ‚owy
                rclpy.time.Time()   # najnowsza transformacja
            )
            pos = t.transform.translation
            self.get_logger().info(
                f'Prawa dÅ‚oÅ„ â†’ base: x={pos.x:.3f}, y={pos.y:.3f}, z={pos.z:.3f}'
            )
        except Exception as e:
            self.get_logger().warn(f'Transformacja niedostÄ™pna: {e}')</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->
                        <div class="section-label">ğŸ—ï¸ CzÄ™Å›Ä‡ 6 â€” Unitree SDK2 (Low-Level Control)</div>

                        <p class="text-stone-600 leading-relaxed">
                            SDK2 umoÅ¼liwia <strong>bezpoÅ›redni</strong> dostÄ™p do silnikÃ³w G1 z pominiÄ™ciem ROS 2 â€“ niezbÄ™dny przy implementacji wÅ‚asnych kontrolerÃ³w balansu lub Reinforcement Learning. Komunikuje siÄ™ przez DDS bezpoÅ›rednio z kontrolerem ruchu robota.
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4 my-2">
                            <div class="p-5 bg-stone-900 text-white rounded-xl">
                                <span class="text-amber-400 font-black text-xs uppercase block mb-2">High-Level (Sport Mode)</span>
                                <p class="text-xs opacity-70 leading-relaxed">WysyÅ‚asz komendy semantyczne: â€idÅº do przodu", â€skrÄ™Ä‡ w lewo", â€usiÄ…dÅº". Robot sam dba o balans, kinematykÄ™ i planowanie stÃ³p. Bezpieczne dla poczÄ…tkujÄ…cych.</p>
                            </div>
                            <div class="p-5 bg-stone-900 text-white rounded-xl">
                                <span class="text-amber-400 font-black text-xs uppercase block mb-2">Low-Level (Joint Control)</span>
                                <p class="text-xs opacity-70 leading-relaxed">PeÅ‚na kontrola kaÅ¼dego z 23+ silnikÃ³w: moment obrotowy (Torque), sztywnoÅ›Ä‡ (Kp) i tÅ‚umienie (Kd) per staw. Wymaga wÅ‚asnego kontrolera balansu. Idealne do RL.</p>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">10. Kompilacja SDK2 i Python Wrapper</h3>
                            <div class="code-block">
                                <pre><code>git clone https://github.com/unitreerobotics/unitree_sdk2.git
cd unitree_sdk2 && mkdir build && cd build
cmake .. -DUNITREE_SDK2_PYTHON=ON
make -j$(nproc) && sudo make install

# Instalacja Python bindings
cd python && pip install .

# Weryfikacja
python3 -c "import unitree_sdk2; print('SDK2 OK')"</code></pre>
                            </div>
                        </section>

                        <div class="critical-warn p-5 rounded-xl">
                            <h4 class="font-bold text-red-800">âš ï¸ KolejnoÅ›Ä‡ startu w trybie Low-Level</h4>
                            <p class="text-sm text-red-700">
                                <strong>1.</strong> Robot musi byÄ‡ zawieszony (nogi w powietrzu) lub leÅ¼eÄ‡. &nbsp;
                                <strong>2.</strong> Uruchom skrypt z Kp = 0. &nbsp;
                                <strong>3.</strong> Stopniowo zwiÄ™kszaj Kp przez kilkanaÅ›cie sekund. &nbsp;
                                NagÅ‚e ustawienie wysokiego Kp wzbudza oscylacje, ktÃ³re mogÄ… trwale uszkodziÄ‡ przekÅ‚adnie silnikÃ³w.
                            </p>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">11. Sterowanie stawem kolanowym (Python SDK2)</h3>
                            <p class="text-stone-600 mb-3">Kompletny przykÅ‚ad: ustawienie pozycji stawu kolanowego prawej nogi z kontrolerem PD.</p>
                            <div class="code-block">
                                <pre><code>from unitree_sdk2.common import ChannelFactory
from unitree_sdk2.hg.h1_2_arm_sdk import LowCmd, LowState
from unitree_sdk2.common import ChannelPublisher, ChannelSubscriber
import time, math

# â”€â”€ Inicjalizacja â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ChannelFactory.Initialize(0, "eth0")  # 0 = domyÅ›lny DDS domain

current_state = None

def on_low_state(msg: LowState):
    global current_state
    current_state = msg

sub = ChannelSubscriber("rt/lowstate", LowState)
sub.InitSubscriber(on_low_state)

pub = ChannelPublisher("rt/lowcmd", LowCmd)
pub.InitPublisher()

# â”€â”€ Czekaj na pierwszy stan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
while current_state is None:
    time.sleep(0.01)

# â”€â”€ Parametry kontrolera PD â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TARGET_JOINT = 4   # Indeks: prawe kolano w G1
Kp = 80.0          # SztywnoÅ›Ä‡ pozycyjna [Nm/rad]
Kd = 2.0           # TÅ‚umienie [NmÂ·s/rad]
TARGET_POS = -0.6  # Cel: -0.6 rad (~34Â°)

# â”€â”€ PÄ™tla sterowania 500 Hz â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FREQ = 500
dt = 1.0 / FREQ

for step in range(FREQ * 3):  # 3 sekundy
    cmd = LowCmd()

    # Kopiuj reszty stawÃ³w ze stanu (tryb damping)
    for i in range(len(cmd.motor_cmd)):
        cmd.motor_cmd[i].mode = 0x01
        cmd.motor_cmd[i].q   = current_state.motor_state[i].q
        cmd.motor_cmd[i].kp  = 0.0
        cmd.motor_cmd[i].kd  = 2.0
        cmd.motor_cmd[i].tau = 0.0

    # Cel dla stawu kolanowego
    cmd.motor_cmd[TARGET_JOINT].q   = TARGET_POS
    cmd.motor_cmd[TARGET_JOINT].kp  = Kp
    cmd.motor_cmd[TARGET_JOINT].kd  = Kd
    cmd.motor_cmd[TARGET_JOINT].tau = 0.0

    pub.Write(cmd)
    time.sleep(dt)

print("Gotowe â€“ powrÃ³t do trybu damping.")</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">12. Odczyt peÅ‚nego stanu robota (Python SDK2)</h3>
                            <p class="text-stone-600 mb-3">Subskrybent zbierajÄ…cy pozycje wszystkich stawÃ³w, dane IMU i styki stÃ³p jednoczeÅ›nie.</p>
                            <div class="code-block">
                                <pre><code">from unitree_sdk2.common import ChannelFactory, ChannelSubscriber
from unitree_sdk2.idl.default import unitree_go_msg_dds__SportModeState_
import time, numpy as np

ChannelFactory.Initialize(0, "eth0")

def on_state_msg(msg):
    # Pozycje stawÃ³w jako numpy array
    q = np.array(msg.motor_state[i].q for i in range(23)])
    # Dane IMU
    rpy  = msg.imu_state.rpy      # [roll, pitch, yaw] w radianach
    gyro = msg.imu_state.gyroscope # prÄ™dkoÅ›ci kÄ…towe [rad/s]
    acc  = msg.imu_state.accelerometer  # przyspieszenia [m/sÂ²]
    # Kontakt stÃ³p (bool)
    foot_contact = msg.foot_force  # siÅ‚y nacisku [N]

    print(f"RPY: {np.degrees(rpy).round(2)} deg")
    print(f"Kontakt stÃ³p: {foot_contact}")
    print(f"Pierwszych 6 kÄ…tÃ³w: {q[:6].round(4)} rad")

sub = ChannelSubscriber("rt/sportmodestate",
                         unitree_go_msg_dds__SportModeState_)
sub.InitSubscriber(on_state_msg)

print("NasÅ‚uchiwanie stanu G1... (Ctrl+C aby zakoÅ„czyÄ‡)")
while True:
    time.sleep(1)</code></pre>
                            </div>
                        </section>

                        <div class="info-box p-5 rounded-xl">
                            <h4 class="font-bold text-green-800">âœ… Kiedy uÅ¼ywaÄ‡ ROS 2, a kiedy SDK2?</h4>
                            <p class="text-sm text-green-700 leading-relaxed">
                                UÅ¼ywaj <strong>ROS 2</strong> gdy potrzebujesz integracji z nav2, MoveIt, rviz2 lub piszesz moduÅ‚owy system wielu wÄ™zÅ‚Ã³w. UÅ¼ywaj <strong>SDK2</strong> bezpoÅ›rednio gdy piszesz wÅ‚asny kontroler low-level, trenujesz polityki RL wymagajÄ…ce pÄ™tli 500Hz+, lub gdy latencja ROS 2 bridge jest nieakceptowalna dla Twojej aplikacji.
                            </p>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 3: VISION AI & PERCEPTION
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'ai',
                title: 'Vision AI & Perception',
                icon: 'ğŸ‘ï¸',
                links: [
                    {name: 'MediaPipe', url: 'https://ai.google.dev/edge/mediapipe/solutions/guide?hl=pl'},
                    {name: 'OpenVLA', url: 'https://openvla.github.io/'},
                    {name: 'UniForm WMA', url: 'https://github.com/unitreerobotics/unifolm-world-model-action'}
                ],
                description: 'Percepcja wizualna, rozpoznawanie gestÃ³w i twarzy, a takÅ¼e uczenie nawigacji w przestrzeni z uÅ¼yciem modeli VLA (Vision-Language-Action) i WMA (World Model Action).',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- â”€â”€â”€ KLASYCZNA PERCEPCJA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <div class="section-label">ğŸ‘ï¸ CzÄ™Å›Ä‡ 1 â€” Klasyczna percepcja: MediaPipe & DeepFace</div>

                        <p class="text-stone-600 leading-relaxed">
                            Klasyczny pipeline percepcji Å‚Ä…czy lekkie modele czasu rzeczywistego do Å›ledzenia ciaÅ‚a i twarzy. DziaÅ‚a na CPU, nie wymaga trenowania i dostarcza robotowi informacji o ludziach w otoczeniu w ciÄ…gu milisekund.
                        </p>

                        <div class="pro-tip p-5 rounded-xl">
                            <h4 class="font-bold text-blue-800">Redukcja latencji dla strumienia wideo</h4>
                            <p class="text-sm text-blue-700">UÅ¼yj bufora koÅ‚owego (circular buffer) dla klatek wideo. Przetwarzaj tylko najnowszÄ… klatkÄ™, odrzucajÄ…c te, ktÃ³re â€utknÄ™Å‚y" w kolejce procesora obrazu. Dla G1 zalecany FPS analizy to 15â€“30 Hz.</p>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja Å›rodowiska wizyjnego</h3>
                            <div class="code-block">
                                <pre><code># StwÃ³rz izolowane Å›rodowisko (unika konfliktÃ³w z ROS2)
python3 -m venv ~/g1_env && source ~/g1_env/bin/activate

# Instalacja MediaPipe i DeepFace
pip install tensorflow deepface mediapipe opencv-python tf-keras

# Opcjonalnie â€“ akceleracja GPU (CUDA 11.8)
pip install "tensorflow[and-cuda]"

# Test kamery
python3 -c "import cv2; cap=cv2.VideoCapture(0); print('Kamera:', cap.isOpened())"</code></pre>
                            </div>
                        </section>

                        <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                            <div class="p-5 bg-white border rounded-xl shadow-sm">
                                <h4 class="font-bold text-amber-600 uppercase text-xs mb-2">DeepFace</h4>
                                <p class="text-xs text-stone-500 leading-relaxed">Analiza emocji, wieku i identyfikacja toÅ¼samoÅ›ci w czasie rzeczywistym z kamer robota. Backend: VGG-Face, Facenet, ArcFace. ObsÅ‚uguje 7 emocji i weryfikacjÄ™ 1:N.</p>
                            </div>
                            <div class="p-5 bg-white border rounded-xl shadow-sm">
                                <h4 class="font-bold text-amber-600 uppercase text-xs mb-2">MediaPipe Holistic</h4>
                                <p class="text-xs text-stone-500 leading-relaxed">33 punkty postawy ciaÅ‚a + 21 punktÃ³w kaÅ¼dej dÅ‚oni + 468 punktÃ³w twarzy w jednym przebiegu. DziaÅ‚a w 30ms na CPU. Podstawa dla teleoperacji gestami.</p>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Åšledzenie gestÃ³w w czasie rzeczywistym (MediaPipe)</h3>
                            <p class="text-stone-600 text-sm mb-3">PoniÅ¼szy skrypt wyodrÄ™bnia kÄ…t Å‚okcia prawego ramienia czÅ‚owieka i wysyÅ‚a go jako cel dla odpowiedniego stawu G1 â€“ podstawa teleoperacji.</p>
                            <div class="code-block">
                                <pre><code>import cv2, math, rclpy
import mediapipe as mp
from std_msgs.msg import Float64

rclpy.init()
node = rclpy.create_node('gesture_teleop')
pub  = node.create_publisher(Float64, '/g1/right_elbow_target', 10)

mp_pose = mp.solutions.pose
pose    = mp_pose.Pose(min_detection_confidence=0.6)

def angle_between(a, b, c):
    """KÄ…t w punkcie b wyznaczony przez punkty a, b, c [stopnie]"""
    ab = [a.x - b.x, a.y - b.y]
    cb = [c.x - b.x, c.y - b.y]
    cos_angle = (ab[0]*cb[0] + ab[1]*cb[1]) / (
        math.hypot(*ab) * math.hypot(*cb) + 1e-9)
    return math.degrees(math.acos(max(-1.0, min(1.0, cos_angle))))

cap = cv2.VideoCapture(0)
while cap.isOpened():
    ret, frame = cap.read()
    if not ret: break
    results = pose.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    if results.pose_landmarks:
        lm = results.pose_landmarks.landmark
        # Indeksy MediaPipe: ramiÄ™=12, Å‚okieÄ‡=14, nadgarstek=16 (prawa strona)
        shoulder = lm[mp_pose.PoseLandmark.RIGHT_SHOULDER]
        elbow    = lm[mp_pose.PoseLandmark.RIGHT_ELBOW]
        wrist    = lm[mp_pose.PoseLandmark.RIGHT_WRIST]

        deg = angle_between(shoulder, elbow, wrist)
        rad = math.radians(deg)

        msg = Float64()
        msg.data = rad
        pub.publish(msg)

        cv2.putText(frame, f'ÅokieÄ‡: {deg:.1f}Â°', (20, 50),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 120), 2)

    cv2.imshow('G1 Gesture Teleop', frame)
    if cv2.waitKey(1) & 0xFF == 27: break

cap.release()
rclpy.shutdown()</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Rozpoznawanie twarzy i nastroju (DeepFace)</h3>
                            <div class="code-block">
                                <pre><code>from deepface import DeepFace
import cv2

# Rejestracja twarzy uÅ¼ytkownika do bazy (jednorazowo)
# Zapisz zdjÄ™cie: ~/faces/jan_kowalski.jpg

def analyze_frame(frame):
    try:
        # Analiza emocji + weryfikacja toÅ¼samoÅ›ci
        analysis = DeepFace.analyze(
            frame, actions=['emotion', 'age'], enforce_detection=False)
        identity = DeepFace.find(
            frame, db_path='~/faces', enforce_detection=False)

        emotion = analysis[0]['dominant_emotion']
        age     = analysis[0]['age']
        name    = identity[0].iloc[0]['identity'] if len(identity[0]) > 0 \
                  else 'nieznany'

        print(f"Osoba: {name} | Wiek: ~{age} | NastrÃ³j: {emotion}")
        # G1 dobiera styl powitania do nastroju rozmÃ³wcy
        return {'name': name, 'age': age, 'emotion': emotion}

    except Exception as e:
        return {'name': 'nieznany', 'emotion': 'unknown'}

cap = cv2.VideoCapture(0)
ret, frame = cap.read()
result = analyze_frame(frame)
print(result)</code></pre>
                            </div>
                        </section>

                        <div class="bg-stone-100 p-4 rounded-xl text-sm">
                            <strong>Tip:</strong> UÅ¼ywaj <code>virtualenv</code> (np. Conda), aby unikaÄ‡ konfliktÃ³w miÄ™dzy TensorFlow a bibliotekami systemowymi ROS2. Dla DeepFace z akceleracjÄ… GPU ustaw <code>CUDA_VISIBLE_DEVICES=0</code> przed startem wÄ™zÅ‚a.
                        </div>

                        <!-- â”€â”€â”€ VLA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ§  CzÄ™Å›Ä‡ 2 â€” Vision-Language-Action: OpenVLA</div>

                        <!-- Kontekst konceptualny -->
                        <div class="bg-stone-950 rounded-2xl p-7 text-white">
                            <p class="text-amber-400 text-[10px] font-black uppercase tracking-widest mb-3">Czym jest VLA?</p>
                            <h3 class="text-xl font-black mb-3 leading-snug">Robot rozumie obraz i tekst,<br>i sam generuje akcje motoryczne.</h3>
                            <p class="text-stone-300 text-sm leading-relaxed">
                                Modele <strong>Vision-Language-Action (VLA)</strong> to nastÄ™pny krok po Imitation Learning. Zamiast mapowaÄ‡ surowe piksele na ruchy (jak LeRobot), model VLA rozumie <em>co widzi</em> (Vision Encoder), <em>co mu powiedziano</em> (Language Instruction) i generuje ciÄ…g akcji robotycznych. MoÅ¼na powiedzieÄ‡ robotowi: <em>â€PoÅ‚Ã³Å¼ czerwony kubek na biaÅ‚ej serwetce"</em> â€” bez programowania trajektorii.
                            </p>
                            <div class="grid grid-cols-3 gap-3 mt-5">
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-xl mb-1">ğŸ“·</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">Vision Encoder</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">Przetwarza obraz z kamery na tokeny wizualne</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-xl mb-1">ğŸ’¬</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">Language Encoder</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">Tokenizuje instrukcjÄ™ tekstowÄ… operatora</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-[21px] mb-1">ğŸ¦¾</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">Action Head</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">Generuje kÄ…ty stawÃ³w / Î”pose ramienia</p>
                                </div>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Architektura OpenVLA</h3>
                            <p class="text-stone-600 leading-relaxed mb-4">
                                <strong>OpenVLA</strong> (Stanford + Berkeley, 2024) to pierwszy w peÅ‚ni open-source model VLA o 7 miliardach parametrÃ³w, oparty na <strong>Llama 2</strong> jako szkielecie jÄ™zykowym i <strong>SigLIP + DinoV2</strong> jako enkoderem wizualnym. Model jest wytrenowany na zbiorze <em>Open X-Embodiment</em> zawierajÄ…cym dane z 22 rÃ³Å¼nych platform robotycznych â€” w tym robotÃ³w manipulacyjnych zbliÅ¼onych do konfiguracji ramion G1.
                            </p>

                            <div class="overflow-x-auto rounded-xl border border-stone-100 mb-4">
                                <table class="w-full text-xs border-collapse">
                                    <thead>
                                        <tr class="bg-stone-900 text-amber-400">
                                            <th class="text-left px-4 py-3 font-black">Komponent</th>
                                            <th class="text-left px-4 py-3 font-black">Model</th>
                                            <th class="text-left px-4 py-3 font-black">Rola</th>
                                        </tr>
                                    </thead>
                                    <tbody class="divide-y divide-stone-100 bg-white">
                                        <tr class="hover:bg-stone-50">
                                            <td class="px-4 py-3 font-bold">Backbone jÄ™zykowy</td>
                                            <td class="px-4 py-3 font-mono text-blue-600">Llama 2 (7B)</td>
                                            <td class="px-4 py-3 text-stone-600">Przetwarza sekwencjÄ™ tokenÃ³w wizualnych + tekstowych, generuje tokeny akcji</td>
                                        </tr>
                                        <tr class="hover:bg-stone-50">
                                            <td class="px-4 py-3 font-bold">Enkoder wizualny</td>
                                            <td class="px-4 py-3 font-mono text-blue-600">SigLIP + DINOv2</td>
                                            <td class="px-4 py-3 text-stone-600">Ekstrakcja cech z obrazu kamery (224Ã—224), fuzja lokalna i globalna</td>
                                        </tr>
                                        <tr class="hover:bg-stone-50">
                                            <td class="px-4 py-3 font-bold">GÅ‚owica akcji</td>
                                            <td class="px-4 py-3 font-mono text-blue-600">Discrete tokenizer</td>
                                            <td class="px-4 py-3 text-stone-600">7-wymiarowy wektor akcji [Î”x, Î”y, Î”z, Î”roll, Î”pitch, Î”yaw, gripper]</td>
                                        </tr>
                                        <tr class="hover:bg-stone-50">
                                            <td class="px-4 py-3 font-bold">Dataset treningowy</td>
                                            <td class="px-4 py-3 font-mono text-blue-600">Open X-Embodiment</td>
                                            <td class="px-4 py-3 text-stone-600">970k epizodÃ³w, 22 platformy robotyczne, 527 zadaÅ„</td>
                                        </tr>
                                    </tbody>
                                </table>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja i uruchomienie OpenVLA</h3>
                            <div class="code-block">
                                <pre><code># Klonowanie repozytorium OpenVLA
git clone https://github.com/openvla/openvla.git
cd openvla

# Instalacja zaleÅ¼noÅ›ci (wymaga CUDA 11.8+, Python 3.10+)
pip install -e ".[train]"
pip install flash-attn --no-build-isolation  # ~3x szybszy forward pass

# Pobranie wag z Hugging Face (7B â€“ ~14 GB)
# Automatyczne przy pierwszym uÅ¼yciu lub rÄ™cznie:
huggingface-cli download openvla/openvla-7b \
    --local-dir ./checkpoints/openvla-7b</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Inferencja: obraz z kamery G1 + instrukcja tekstowa â†’ akcja</h3>
                            <div class="code-block">
                                <pre><code>#!/usr/bin/env python3
# WÄ™zeÅ‚ ROS 2: OpenVLA generuje komendy dla ramienia G1
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from trajectory_msgs.msg import JointTrajectory, JointTrajectoryPoint
from cv_bridge import CvBridge
from PIL import Image as PILImage
from transformers import AutoModelForVision2Seq, AutoProcessor
import torch, numpy as np

INSTRUCTION = "pick up the red box and place it in the container"
DEVICE = "cuda:0"

class OpenVLAController(Node):
    def __init__(self):
        super().__init__('openvla_controller')

        # Wczytaj model (pierwsze uruchomienie â€“ pobiera wagi)
        self.get_logger().info('Åadowanie OpenVLA 7B...')
        self.processor = AutoProcessor.from_pretrained(
            "openvla/openvla-7b", trust_remote_code=True)
        self.model = AutoModelForVision2Seq.from_pretrained(
            "openvla/openvla-7b",
            attn_implementation="flash_attention_2",
            torch_dtype=torch.bfloat16,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        ).to(DEVICE)
        self.get_logger().info('âœ… OpenVLA gotowy!')

        self.bridge = CvBridge()

        # Subskrybuj obraz z kamery gÅ‚owowej G1
        self.create_subscription(
            Image, '/camera/color/image_raw', self.on_image, 5)

        # Publikuj cel dla kontrolera ramienia (MoveIt / SDK2)
        self.action_pub = self.create_publisher(
            JointTrajectory, '/right_arm/joint_trajectory', 10)

        # Metryki statystyk
        self.action_stats = {
            "mean": np.zeros(7),
            "std":  np.ones(7)
        }

    def on_image(self, msg):
        # Konwersja ROS Image â†’ PIL Image
        cv_img  = self.bridge.imgmsg_to_cv2(msg, desired_encoding='rgb8')
        pil_img = PILImage.fromarray(cv_img)

        # Przygotuj prompt w formacie OpenVLA
        prompt = f"In: What action should the robot take to {INSTRUCTION}?\nOut:"

        inputs = self.processor(prompt, pil_img).to(DEVICE, dtype=torch.bfloat16)

        # Inferencja â€“ generuj 7-wymiarowÄ… akcjÄ™
        with torch.no_grad():
            action_tokens = self.model.predict_action(
                **inputs,
                unnorm_key="bridge_orig",  # klucz normalizacji datasetu
                do_sample=False
            )

        action = action_tokens.cpu().numpy()  # [Î”x, Î”y, Î”z, Î”r, Î”p, Î”y, gripper]

        self.get_logger().info(
            f'Akcja VLA â†’ Î”pos: {action[:3].round(4)} | '
            f'Î”orient: {action[3:6].round(4)} | '
            f'gripper: {"zamknij" if action[6] < 0 else "otwÃ³rz"}'
        )

        # WyÅ›lij jako komendÄ™ trajektorii (uproszczone mapowanie)
        self._publish_arm_command(action)

    def _publish_arm_command(self, action):
        traj = JointTrajectory()
        traj.joint_names = [
            'right_shoulder_pitch', 'right_shoulder_roll',
            'right_shoulder_yaw',   'right_elbow',
            'right_wrist_roll',     'right_wrist_pitch',
            'right_wrist_yaw'
        ]
        pt = JointTrajectoryPoint()
        # action[:6] to delta-pozycje â€“ dodaj do aktualnych kÄ…tÃ³w
        pt.positions = action[:6].tolist()
        pt.time_from_start.sec = 0
        pt.time_from_start.nanosec = 500_000_000  # 0.5s
        traj.points = [pt]
        self.action_pub.publish(traj)

def main():
    rclpy.init()
    rclpy.spin(OpenVLAController())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Fine-tuning OpenVLA na wÅ‚asnych danych G1</h3>
                            <p class="text-stone-600 mb-3 text-sm">
                                Generalistyczny OpenVLA moÅ¼e nie rozumieÄ‡ specyfiki kinematyki G1. Fine-tuning na kilkuset wÅ‚asnych epizodach teleoperacji (nagranych przez LeRobot) drastycznie poprawia precyzjÄ™.
                            </p>
                            <div class="code-block">
                                <pre><code># Fine-tuning z LoRA (efektywny â€“ trenuje tylko ~1% parametrÃ³w)
# Wymaga RLDS-sformatowanych danych (standard Open X-Embodiment)
torchrun --standalone --nnodes=1 --nproc-per-node=2 \
    vla-scripts/finetune.py \
    --vla_path "openvla/openvla-7b" \
    --data_root_dir ~/datasets/g1_episodes \
    --dataset_name "g1_pick_place" \
    --run_root_dir ./checkpoints \
    --adapter_tmp_dir ./adapters \
    --lora_rank 32 \
    --batch_size 16 \
    --grad_accumulation_steps 2 \
    --learning_rate 5e-4 \
    --image_aug true \
    --wandb_project "g1-vla-finetune" \
    --wandb_entity "$USER"</code></pre>
                            </div>
                        </section>

                        <div class="critical-warn p-5 rounded-xl">
                            <h4 class="font-bold text-red-800">Wymagania sprzÄ™towe OpenVLA</h4>
                            <p class="text-sm text-red-700">
                                Inferencja modelu 7B wymaga <strong>min. 16 GB VRAM</strong> (RTX 4080 / A4000). Z <code>bfloat16</code> i Flash Attention 2 czas generowania jednej akcji wynosi ~80ms na RTX 4090. Do fine-tuningu z LoRA potrzeba 2Ã— GPU (24 GB kaÅ¼da) lub pojedynczej A100 (40 GB). RozwaÅ¼ deployment na dedykowanym serwerze inference, a nie na komputerze pokÅ‚adowym G1.
                            </p>
                        </div>

                        <!-- â”€â”€â”€ WMA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸŒ CzÄ™Å›Ä‡ 3 â€” World Model Action: UniForm WMA (Unitree)</div>

                        <!-- Kontekst konceptualny -->
                        <div class="bg-stone-950 rounded-2xl p-7 text-white">
                            <p class="text-amber-400 text-[10px] font-black uppercase tracking-widest mb-3">Czym jest World Model Action?</p>
                            <h3 class="text-xl font-black mb-3 leading-snug">Robot buduje wewnÄ™trzny model Å›wiata<br>i symuluje przyszÅ‚oÅ›Ä‡ przed dziaÅ‚aniem.</h3>
                            <p class="text-stone-300 text-sm leading-relaxed mb-4">
                                <strong>VLA</strong> (jak OpenVLA) mapuje obraz + instrukcjÄ™ bezpoÅ›rednio na akcjÄ™ â€” to podejÅ›cie reaktywne. <strong>WMA (World Model Action)</strong> idzie krok dalej: model najpierw buduje <em>latentnÄ… reprezentacjÄ™ stanu Å›wiata</em>, a nastÄ™pnie <em>przewiduje przyszÅ‚e stany</em> dla rÃ³Å¼nych sekwencji akcji, wybierajÄ…c tÄ™, ktÃ³ra prowadzi do celu. Daje to robotowi zdolnoÅ›Ä‡ do planowania na kilka krokÃ³w naprzÃ³d bez fizycznych prÃ³b i bÅ‚Ä™dÃ³w.
                            </p>
                            <div class="grid grid-cols-2 gap-4 mt-2">
                                <div class="p-4 bg-stone-800 rounded-xl">
                                    <p class="text-[10px] font-black uppercase text-amber-400 mb-2">PodejÅ›cie VLA (reaktywne)</p>
                                    <p class="text-xs text-stone-300 font-mono">obraz + tekst â†’ akcja</p>
                                    <p class="text-[10px] text-stone-500 mt-1">Decyzja na podstawie bieÅ¼Ä…cej klatki. Brak planowania w przyszÅ‚oÅ›Ä‡.</p>
                                </div>
                                <div class="p-4 bg-stone-700 rounded-xl border border-amber-500/30">
                                    <p class="text-[10px] font-black uppercase text-amber-400 mb-2">PodejÅ›cie WMA (modelowanie Å›wiata)</p>
                                    <p class="text-xs text-stone-300 font-mono">obraz â†’ model Å›wiata â†’ plan â†’ akcja</p>
                                    <p class="text-[10px] text-stone-500 mt-1">Model przewiduje skutki akcji, wybiera najlepszÄ… sekwencjÄ™.</p>
                                </div>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">UniForm â€” architektura Unitree WMA</h3>
                            <p class="text-stone-600 leading-relaxed mb-4">
                                <strong>UniForm</strong> (Unitree Robotics, 2024) to framework World Model Action zaprojektowany specjalnie dla robotÃ³w humanoidalnych, w tym G1. ÅÄ…czy trzy komponenty: <strong>World Model</strong> (predykcja kolejnych ramek wideo w przestrzeni latentnej), <strong>Action Decoder</strong> (mapowanie celu na sekwencjÄ™ ruchÃ³w caÅ‚ego ciaÅ‚a) oraz <strong>Planner</strong> oparty na drzewach MCTS lub Model Predictive Control (MPC).
                            </p>

                            <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-5">
                                <div class="p-5 bg-stone-50 rounded-xl border border-stone-100">
                                    <div class="text-2xl mb-2">ğŸ—ºï¸</div>
                                    <span class="text-xs font-black uppercase text-stone-700 block mb-1">World Model</span>
                                    <p class="text-xs text-stone-500 leading-relaxed">Enkoder VAE tworzy latentny stan Å›wiata <em>z</em>, przewiduje <em>z</em>â‚œâ‚Šâ‚™ dla zadanej akcji. Trenowany na milionach klatek wideo z robotami.</p>
                                </div>
                                <div class="p-5 bg-stone-50 rounded-xl border border-stone-100">
                                    <div class="text-2xl mb-2">ğŸ“</div>
                                    <span class="text-xs font-black uppercase text-stone-700 block mb-1">Action Decoder</span>
                                    <p class="text-xs text-stone-500 leading-relaxed">Transformer dekoduje latentny cel na peÅ‚nÄ… sekwencjÄ™ kÄ…tÃ³w stawÃ³w dla wszystkich 23+ silnikÃ³w G1 z uwzglÄ™dnieniem kinematyki humanoidalnej.</p>
                                </div>
                                <div class="p-5 bg-stone-50 rounded-xl border border-stone-100">
                                    <div class="text-2xl mb-2">ğŸ”®</div>
                                    <span class="text-xs font-black uppercase text-stone-700 block mb-1">Planner (MPC)</span>
                                    <p class="text-xs text-stone-500 leading-relaxed">Model Predictive Control: na kaÅ¼dym kroku generuje N kandydatÃ³w akcji, symuluje je w modelu Å›wiata, wybiera optymalnÄ… sekwencjÄ™ â€“ i wykonuje tylko pierwszy krok.</p>
                                </div>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja UniForm WMA</h3>
                            <div class="code-block">
                                <pre><code># Klonowanie repozytorium
git clone https://github.com/unitreerobotics/unifolm-world-model-action.git
cd unifolm-world-model-action

# Instalacja zaleÅ¼noÅ›ci (PyTorch 2.1+, CUDA 12.1)
conda create -n uniform python=3.10 -y
conda activate uniform
pip install torch==2.1.0 torchvision --index-url https://download.pytorch.org/whl/cu121
pip install -r requirements.txt

# Pobranie wag pre-trained world model
python scripts/download_weights.py --model world_model_g1_v1

# Weryfikacja instalacji
python -c "import uniform; print(uniform.__version__)"</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Trening modelu Å›wiata na wÅ‚asnych danych G1</h3>
                            <p class="text-stone-600 mb-3 text-sm">
                                World Model uczy siÄ™ przewidywaÄ‡ przyszÅ‚e klatki wideo na podstawie historii obserwacji i wykonywanych akcji. Im wiÄ™cej nagranych epizodÃ³w, tym lepsza jakoÅ›Ä‡ predykcji.
                            </p>
                            <div class="code-block">
                                <pre><code># â”€â”€ Krok 1: Przygotowanie datasetu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Format: katalog z epizodami, kaÅ¼dy epizod = {frames/, actions.npy}
python scripts/prepare_dataset.py \
    --input_dir ~/datasets/g1_rosbag_sessions \
    --output_dir ~/datasets/g1_uniform_format \
    --image_size 224 \
    --fps 30

# â”€â”€ Krok 2: Trening World Model (VAE + Transformer) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python train_world_model.py \
    --config configs/g1_world_model.yaml \
    --data_path ~/datasets/g1_uniform_format \
    --output_dir ./checkpoints/wm_g1_v1 \
    --batch_size 32 \
    --num_epochs 100 \
    --lr 1e-4 \
    --latent_dim 512 \
    --pred_horizon 16 \
    --gpus 0,1

# â”€â”€ Krok 3: Trening Action Decoder â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
python train_action_decoder.py \
    --world_model_ckpt ./checkpoints/wm_g1_v1/best.pt \
    --data_path ~/datasets/g1_uniform_format \
    --output_dir ./checkpoints/ad_g1_v1 \
    --num_joints 23 \
    --chunk_size 50</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Deployment: Autonomiczna nawigacja przez WMA</h3>
                            <p class="text-stone-600 mb-3 text-sm">
                                PoniÅ¼szy wÄ™zeÅ‚ ROS 2 uruchamia pÄ™tlÄ™ MPC â€” w kaÅ¼dym kroku model Å›wiata symuluje kilka moÅ¼liwych Å›cieÅ¼ek i wybiera optymalnÄ… dla osiÄ…gniÄ™cia celu podanego jako obraz docelowy lub instrukcja tekstowa.
                            </p>
                            <div class="code-block">
                                <pre><code>#!/usr/bin/env python3
# WÄ™zeÅ‚ ROS 2: UniForm WMA â€“ autonomiczna nawigacja przez model Å›wiata
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
import torch, numpy as np
from PIL import Image as PILImage

# Import komponentÃ³w UniForm
from uniform.world_model import WorldModel
from uniform.action_decoder import ActionDecoder
from uniform.planner import MPCPlanner

DEVICE = "cuda:0"
GOAL_IMAGE_PATH = "/home/g1/goals/target_room_state.jpg"  # cel jako obraz

class WMANavigator(Node):
    def __init__(self):
        super().__init__('wma_navigator')
        self.bridge = CvBridge()

        # Wczytaj komponenty WMA
        self.get_logger().info('Åadowanie UniForm World Model...')
        self.world_model = WorldModel.from_pretrained(
            './checkpoints/wm_g1_v1/best.pt').to(DEVICE)
        self.action_decoder = ActionDecoder.from_pretrained(
            './checkpoints/ad_g1_v1/best.pt').to(DEVICE)

        # Planner MPC: 64 kandydatÃ³w akcji, horyzont 8 krokÃ³w
        self.planner = MPCPlanner(
            world_model=self.world_model,
            action_decoder=self.action_decoder,
            num_candidates=64,
            horizon=8,
            device=DEVICE
        )

        # Cel: zaÅ‚aduj docelowy stan Å›wiata jako latent
        goal_img = PILImage.open(GOAL_IMAGE_PATH).resize((224, 224))
        goal_tensor = torch.tensor(np.array(goal_img)).permute(2,0,1) \
                           .float().unsqueeze(0).to(DEVICE) / 255.0
        with torch.no_grad():
            self.goal_latent = self.world_model.encode(goal_tensor)
        self.get_logger().info('âœ… WMA gotowy. NawigujÄ™ do celu...')

        # Historia obserwacji (bufor 8 ostatnich klatek)
        self.obs_buffer = []

        self.create_subscription(
            Image, '/camera/color/image_raw', self.on_obs, 10)

        # Publikuj komendy do SDK2 / MoveIt
        from unitree_sdk2.common import ChannelFactory, ChannelPublisher
        ChannelFactory.Initialize(0, "eth0")
        self.sdk_pub = ChannelPublisher("rt/lowcmd")
        self.sdk_pub.InitPublisher()

    def on_obs(self, msg):
        frame = self.bridge.imgmsg_to_cv2(msg, 'rgb8')
        pil   = PILImage.fromarray(frame).resize((224, 224))
        t     = torch.tensor(np.array(pil)).permute(2,0,1) \
                     .float().unsqueeze(0).to(DEVICE) / 255.0

        with torch.no_grad():
            obs_latent = self.world_model.encode(t)

        # Utrzymuj bufor 8 klatek
        self.obs_buffer.append(obs_latent)
        if len(self.obs_buffer) < 4:
            return  # Czekaj na minimum historii
        if len(self.obs_buffer) > 8:
            self.obs_buffer.pop(0)

        obs_seq = torch.cat(self.obs_buffer, dim=0).unsqueeze(0)

        # MPC: znajdÅº optymalnÄ… sekwencjÄ™ akcji
        with torch.no_grad():
            best_actions = self.planner.plan(
                current_obs=obs_seq,
                goal_latent=self.goal_latent
            )

        # Wykonaj pierwszy krok optymalnej sekwencji
        first_action = best_actions[0].cpu().numpy()  # [23 kÄ…ty stawÃ³w]

        dist_to_goal = self.planner.last_goal_distance
        self.get_logger().info(
            f'WMA plan | dystans do celu: {dist_to_goal:.4f} | '
            f'akcja[0:6]: {first_action[:6].round(3)}'
        )

        self._send_joint_command(first_action)

    def _send_joint_command(self, joint_angles: np.ndarray):
        """WyÅ›lij kÄ…ty stawÃ³w przez SDK2 Low-Level"""
        from unitree_sdk2.hg.h1_2_arm_sdk import LowCmd
        cmd = LowCmd()
        for i, angle in enumerate(joint_angles[:23]):
            cmd.motor_cmd[i].q   = float(angle)
            cmd.motor_cmd[i].kp  = 50.0
            cmd.motor_cmd[i].kd  = 2.0
            cmd.motor_cmd[i].tau = 0.0
        self.sdk_pub.Write(cmd)

def main():
    rclpy.init()
    rclpy.spin(WMANavigator())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <!-- PorÃ³wnanie podejÅ›Ä‡ -->
                        <hr class="section-divider">
                        <div class="section-label">âš–ï¸ CzÄ™Å›Ä‡ 4 â€” PorÃ³wnanie podejÅ›Ä‡: MediaPipe vs VLA vs WMA</div>

                        <div class="overflow-x-auto rounded-xl border border-stone-100">
                            <table class="w-full text-xs border-collapse">
                                <thead>
                                    <tr class="bg-stone-900 text-amber-400">
                                        <th class="text-left px-4 py-3 font-black">Kryterium</th>
                                        <th class="text-left px-4 py-3 font-black">MediaPipe / DeepFace</th>
                                        <th class="text-left px-4 py-3 font-black">OpenVLA (7B)</th>
                                        <th class="text-left px-4 py-3 font-black">UniForm WMA</th>
                                    </tr>
                                </thead>
                                <tbody class="divide-y divide-stone-100 bg-white">
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Latencja</td>
                                        <td class="px-4 py-3 text-green-600 font-bold">&lt;20 ms (CPU)</td>
                                        <td class="px-4 py-3 text-amber-600 font-bold">~80 ms (GPU)</td>
                                        <td class="px-4 py-3 text-red-600 font-bold">~200 ms (GPU)</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">VRAM</td>
                                        <td class="px-4 py-3 text-stone-600">Brak (CPU)</td>
                                        <td class="px-4 py-3 text-stone-600">16 GB+</td>
                                        <td class="px-4 py-3 text-stone-600">8â€“12 GB</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Instrukcje jÄ™zykowe</td>
                                        <td class="px-4 py-3 text-stone-400">âŒ Nie</td>
                                        <td class="px-4 py-3 text-green-600">âœ… Tak (EN/PL po fine-tune)</td>
                                        <td class="px-4 py-3 text-amber-600">âš ï¸ CzÄ™Å›ciowo (cel = obraz)</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Planowanie przyszÅ‚oÅ›ci</td>
                                        <td class="px-4 py-3 text-stone-400">âŒ Nie</td>
                                        <td class="px-4 py-3 text-stone-400">âŒ Reaktywne</td>
                                        <td class="px-4 py-3 text-green-600">âœ… Tak (MPC / MCTS)</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Dane treningowe</td>
                                        <td class="px-4 py-3 text-stone-600">Nie wymagane</td>
                                        <td class="px-4 py-3 text-stone-600">Open X-Embodiment (pre-trained)</td>
                                        <td class="px-4 py-3 text-stone-600">WÅ‚asne nagrania G1 (min. ~500 epizodÃ³w)</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Najlepszy do</td>
                                        <td class="px-4 py-3 text-stone-600">Detekcja twarzy, gesty, teleoperacja</td>
                                        <td class="px-4 py-3 text-stone-600">Sterowanie ramieniem wg instrukcji, pick &amp; place</td>
                                        <td class="px-4 py-3 text-stone-600">ZÅ‚oÅ¼ona nawigacja, dÅ‚ugohoryzontowe planowanie</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="info-box p-5 rounded-xl">
                            <h4 class="font-bold text-green-800">âœ… Zalecana architektura hybrydowa dla G1</h4>
                            <p class="text-sm text-green-700 leading-relaxed">
                                Najlepsze rezultaty daje poÅ‚Ä…czenie wszystkich trzech warstw: <strong>MediaPipe/DeepFace</strong> do szybkiej detekcji i reakcji na czÅ‚owieka (latencja &lt;20ms), <strong>OpenVLA</strong> do interpretacji instrukcji jÄ™zykowych i sterowania chwytakami (latencja ~80ms), oraz <strong>UniForm WMA</strong> do planowania dÅ‚ugoterminowych sekwencji nawigacyjnych (latencja 200ms, uruchamiana rzadziej). Taka architektura jest odporna na awarie pojedynczych komponentÃ³w i skaluje siÄ™ z moÅ¼liwoÅ›ciami GPU.
                            </p>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 4: LeRobot & Imitation Learning
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'lerobot',
                title: 'LeRobot & Imitation Learning',
                icon: 'ğŸ§ ',
                links: [
                    {name: 'Hugging Face LeRobot', url: 'https://huggingface.co/lerobot'},
                    {name: 'GitHub LeRobot', url: 'https://github.com/huggingface/lerobot'}
                ],
                description: 'Framework Hugging Face do uczenia przez naÅ›ladowanie â€“ zbieranie danych z teleoperacji, trening polityk ACT / Diffusion Policy i deployment na Unitree G1.',
                details: `
                    <div class="space-y-6 step-content">

                        <p class="text-stone-600 leading-relaxed">
                            Zamiast rÄ™cznie programowaÄ‡ trajektorie w MoveIt, moÅ¼emy â€pokazaÄ‡" robotowi Unitree G1, jak ma wykonaÄ‡ zadanie. <strong>LeRobot</strong> to framework od Hugging Face, ktÃ³ry demokratyzuje dostÄ™p do <strong>Imitation Learning (IL)</strong>, pozwalajÄ…c na zbieranie danych z teleoperacji i trenowanie modeli AI typu <em>end-to-end</em>.
                        </p>

                        <!-- Kluczowe metody -->
                        <div class="grid grid-cols-1 md:grid-cols-2 gap-5">
                            <div class="p-5 rounded-2xl border-l-4 border-amber-400 bg-amber-50">
                                <span class="inline-block px-2 py-0.5 rounded-full bg-amber-100 text-amber-700 text-[10px] font-black uppercase tracking-wide mb-3">ACT</span>
                                <h4 class="font-bold text-base mb-1">Action Chunking Transformer</h4>
                                <p class="text-sm text-stone-500">Zaawansowany algorytm IL, ktÃ³ry przewiduje caÅ‚e â€pakiety" akcji, redukujÄ…c bÅ‚Ä™dy kumulatywne i zapewniajÄ…c pÅ‚ynnoÅ›Ä‡ ruchu G1.</p>
                            </div>
                            <div class="p-5 rounded-2xl border-l-4 border-blue-400 bg-blue-50">
                                <span class="inline-block px-2 py-0.5 rounded-full bg-blue-100 text-blue-700 text-[10px] font-black uppercase tracking-wide mb-3">Diffusion Policy</span>
                                <h4 class="font-bold text-base mb-1">Polityki Dyfuzyjne</h4>
                                <p class="text-sm text-stone-500">Wykorzystuje modele probabilistyczne (podobnie jak Stable Diffusion) do generowania precyzyjnych i stabilnych trajektorii ramion robota.</p>
                            </div>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 1: Instalacja -->
                        <div class="section-label">ğŸ“¦ Krok 1 â€” Instalacja</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja LeRobot</h3>
                            <p class="text-stone-600 mb-3">Biblioteka wymaga Pythona 3.10+ oraz wsparcia dla akceleracji GPU (CUDA).</p>
                            <div class="code-block">
                                <pre><code>git clone https://github.com/huggingface/lerobot.git
cd lerobot
pip install -e .

# Instalacja zaleÅ¼noÅ›ci GPU dla Unitree G1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install opencv-python pynput</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- KROK 2: Zbieranie danych -->
                        <div class="section-label">ğŸ¥ Krok 2 â€” Zbieranie Danych</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Struktura Datasetu G1</h3>
                            <p class="text-stone-600 mb-4">Proces polega na nagrywaniu pary: <strong>Obraz z kamery robota</strong> + <strong>KÄ…ty silnikÃ³w</strong> (state) wysyÅ‚ane przez czÅ‚owieka podczas teleoperacji.</p>
                            <div class="bg-stone-50 border border-stone-200 rounded-2xl p-5 mb-4">
                                <h4 class="font-bold mb-3 text-stone-700 text-sm">Pola datasetu:</h4>
                                <ul class="space-y-3 text-sm text-stone-600">
                                    <li class="flex items-start gap-2">
                                        <span class="text-amber-500 font-bold">â–¸</span>
                                        <span><code class="bg-stone-100 px-1 rounded">observation.image</code> â€“ Obraz z kamery na gÅ‚owie G1 (3Ã—224Ã—224).</span>
                                    </li>
                                    <li class="flex items-start gap-2">
                                        <span class="text-amber-500 font-bold">â–¸</span>
                                        <span><code class="bg-stone-100 px-1 rounded">observation.state</code> â€“ Wektor pozycji 23+ stawÃ³w robota.</span>
                                    </li>
                                    <li class="flex items-start gap-2">
                                        <span class="text-amber-500 font-bold">â–¸</span>
                                        <span><code class="bg-stone-100 px-1 rounded">action</code> â€“ Pozycje stawÃ³w w nastÄ™pnej klatce (target dla sieci).</span>
                                    </li>
                                </ul>
                            </div>
                            <div class="code-block">
                                <pre><code># Nagrywanie epizodu teleoperacji
python lerobot/scripts/record.py \
    --robot-path unitree_g1 \
    --fps 30 \
    --repo-id $USER/g1-cleaning-task</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- KROK 3: Trening -->
                        <div class="section-label">ğŸ”¬ Krok 3 â€” Trening Polityki AI</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Trening z architekturÄ… ACT</h3>
                            <p class="text-stone-600 mb-3">Uruchomienie treningu z wykorzystaniem architektury ACT dla Twojego datasetu G1:</p>
                            <div class="code-block">
                                <pre><code>python lerobot/scripts/train.py \
    policy=act \
    dataset=unitree_g1_pick_place \
    hydra.run.dir=outputs/train/g1_act_v1 \
    device=cuda \
    wandb.enable=true</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- KROK 4: Deployment -->
                        <div class="section-label">ğŸš€ Krok 4 â€” Deployment na G1</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Inferencja w czasie rzeczywistym</h3>
                            <p class="text-stone-600 mb-3">Po wytrenowaniu modelu, polityka steruje robotem w czasie rzeczywistym, â€widzÄ…c" Å›wiat przez kamerÄ™.</p>
                            <div class="code-block">
                                <pre><code>import lerobot
from unitree_sdk2.python import RobotInterface

# Wczytaj wytrenowanÄ… politykÄ™
policy = lerobot.Policy.from_pretrained("outputs/train/g1_act_v1/checkpoints/last")

# PÄ™tla sterowania
while True:
    obs = robot.get_observations()  # Obraz + Stan
    action = policy.predict(obs)    # Inferencja AI
    robot.set_joint_targets(action) # Ruch silnikÃ³w</code></pre>
                            </div>
                        </section>

                        <!-- Baner podsumowujÄ…cy -->
                        <div class="p-7 rounded-3xl text-white shadow-xl" style="background: linear-gradient(135deg, #f59e0b, #f97316);">
                            <h3 class="text-xl font-black mb-2">Sim2Real &amp; Real2Real</h3>
                            <p class="opacity-90 text-sm leading-relaxed mb-4">
                                LeRobot pozwala nie tylko na zbieranie danych z fizycznego robota (Real2Real), ale rÃ³wnieÅ¼ na importowanie danych z symulacji <strong>Isaac Sim</strong> (Sim2Real), co drastycznie przyspiesza proces nauki chwytania skomplikowanych obiektÃ³w.
                            </p>
                            <div class="flex flex-wrap gap-2">
                                <span class="px-3 py-1 bg-white/20 rounded text-[10px] font-black uppercase">End-to-End Learning</span>
                                <span class="px-3 py-1 bg-white/20 rounded text-[10px] font-black uppercase">Hugging Face Hub Integration</span>
                                <span class="px-3 py-1 bg-white/20 rounded text-[10px] font-black uppercase">ACT / Diffusion Policy</span>
                            </div>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 5: MoveIt 2
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'moveit',
                title: 'MoveIt 2',
                icon: 'ğŸ“',
                links: [
                    {name: 'MoveIt 2 Docs', url: 'https://moveit.picknik.ai/main/index.html'},
                    {name: 'MoveIt Setup Assistant', url: 'https://moveit.picknik.ai/main/doc/examples/setup_assistant/setup_assistant_tutorial.html'}
                ],
                description: 'Najpopularniejszy framework ROS 2 do planowania ruchu, kinematyki odwrotnej i unikania kolizji â€“ zintegrowany z ramionami i dÅ‚oÅ„mi Unitree G1.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- Intro -->
                        <p class="text-stone-600 leading-relaxed">
                            MoveIt 2 to najpopularniejszy framework w systemie <strong>ROS 2 Humble</strong> sÅ‚uÅ¼Ä…cy do planowania ruchu, manipulacji oraz unikania kolizji. W przypadku Unitree G1 zarzÄ…dza kinematykÄ… odwrotnÄ… (IK) dla ramion oraz dÅ‚oni, pozwalajÄ…c na precyzyjne operowanie chwytakami bez rÄ™cznego obliczania kÄ…tÃ³w stawÃ³w.
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-2 gap-5">
                            <div class="p-5 rounded-2xl border-l-4 border-blue-500 bg-white shadow-sm">
                                <h4 class="font-bold text-blue-900 mb-1">Planowanie Trajektorii</h4>
                                <p class="text-sm text-stone-500">Generowanie pÅ‚ynnych Å›cieÅ¼ek od punktu A do B z uwzglÄ™dnieniem ograniczeÅ„ prÄ™dkoÅ›ci i przyspieszenia silnikÃ³w Unitree.</p>
                            </div>
                            <div class="p-5 rounded-2xl border-l-4 border-emerald-500 bg-white shadow-sm">
                                <h4 class="font-bold text-green-900 mb-1">Unikanie Kolizji</h4>
                                <p class="text-sm text-stone-500">Robot â€widzi" wÅ‚asne ciaÅ‚o i otoczenie (poprzez chmurÄ™ punktÃ³w), dziÄ™ki czemu nie uderzy ramieniem w korpus ani przeszkody.</p>
                            </div>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 1: Instalacja -->
                        <div class="section-label">ğŸ“¦ Krok 1 â€” Instalacja</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Przygotowanie Å›rodowiska</h3>
                            <p class="text-stone-600 mb-3">ZakÅ‚adamy, Å¼e posiadasz zainstalowany system <strong>ROS 2 Humble</strong> na Ubuntu 22.04.</p>
                            <div class="code-block">
                                <pre><code># Instalacja MoveIt 2 i narzÄ™dzi wizualizacji
sudo apt update
sudo apt install ros-humble-moveit -y
sudo apt install ros-humble-moveit-visual-tools ros-humble-moveit-resources -y

# Instalacja kontrolerÃ³w ROS 2 Control
sudo apt install ros-humble-ros2-control ros-humble-ros2-controllers -y</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- KROK 2: Konfiguracja -->
                        <div class="section-label">âš™ï¸ Krok 2 â€” MoveIt Setup Assistant</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Generowanie pakietu konfiguracyjnego G1</h3>
                            <p class="text-stone-600 mb-4">Aby skonfigurowaÄ‡ G1, generujemy pakiet <code>moveit_config</code> na podstawie pliku URDF robota.</p>
                            <div class="space-y-3">
                                <div class="flex gap-4 p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                    <div class="font-black text-blue-600 text-lg w-8 shrink-0">01</div>
                                    <div>
                                        <p class="font-bold text-sm mb-1">Uruchomienie asystenta</p>
                                        <code class="text-xs bg-stone-100 px-2 py-0.5 rounded">ros2 launch moveit_setup_assistant setup_assistant.launch.py</code>
                                    </div>
                                </div>
                                <div class="flex gap-4 p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                    <div class="font-black text-blue-600 text-lg w-8 shrink-0">02</div>
                                    <div>
                                        <p class="font-bold text-sm mb-1">Self-Collision Matrix</p>
                                        <p class="text-xs text-stone-500">Wygeneruj macierz kolizji, aby wykluczyÄ‡ stawy, ktÃ³re zawsze siÄ™ dotykajÄ… (np. sÄ…siadujÄ…ce silniki).</p>
                                    </div>
                                </div>
                                <div class="flex gap-4 p-4 bg-white border border-stone-100 rounded-xl shadow-sm">
                                    <div class="font-black text-blue-600 text-lg w-8 shrink-0">03</div>
                                    <div>
                                        <p class="font-bold text-sm mb-1">Planning Groups</p>
                                        <p class="text-xs text-stone-500">Zdefiniuj grupy: <em>left_arm</em>, <em>right_arm</em> oraz <em>waist</em>. Wybierz solver <strong>KDL Kinematics</strong> (lub BioIK dla lepszych wynikÃ³w z humanoidem).</p>
                                    </div>
                                </div>
                            </div>
                        </section>

                        <div class="critical-warn p-5 rounded-xl">
                            <h4 class="font-bold text-red-800">Macierz Kolizji â€“ Virtual Joints</h4>
                            <p class="text-sm text-red-700">W pliku <code>.srdf</code> musisz zdefiniowaÄ‡ â€Virtual Joints" dla torsu robota, aby MoveIt nie traktowaÅ‚ bazy G1 jako przeszkody zewnÄ™trznej. Dla G1 zaleca siÄ™ solver <strong>BioIK</strong> lub <strong>TRAC-IK</strong> zamiast standardowego KDL.</p>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 3: API -->
                        <div class="section-label">ğŸ’» Krok 3 â€” MoveGroup API</div>
                        <section>
                            <h3 class="text-xl font-bold mb-3">Sterowanie ramieniem w przestrzeni kartezjaÅ„skiej (C++)</h3>
                            <p class="text-stone-600 mb-3">PrzykÅ‚ad skryptu, ktÃ³ry przesuwa ramiÄ™ G1 do okreÅ›lonej pozy:</p>
                            <div class="code-block">
                                <pre><code>#include &lt;moveit/move_group_interface/move_group_interface.h&gt;

auto move_group_interface = moveit::planning_interface::MoveGroupInterface(node, "right_arm");

// Ustawienie docelowej pozycji (x, y, z) i orientacji (quaternion)
geometry_msgs::msg::Pose target_pose;
target_pose.orientation.w = 1.0;
target_pose.position.x = 0.3;
target_pose.position.y = -0.2;
target_pose.position.z = 0.5;

move_group_interface.setPoseTarget(target_pose);

// Planowanie i wykonanie ruchu
moveit::planning_interface::MoveGroupInterface::Plan my_plan;
bool success = (move_group_interface.plan(my_plan) ==
                moveit::core::MoveItErrorCode::SUCCESS);

if (success) {
    move_group_interface.execute(my_plan);
}</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- KROK 4: MoveIt Servo -->
                        <div class="section-label">âš¡ Krok 4 â€” Sterowanie Real-Time</div>
                        <div class="p-7 bg-blue-600 rounded-3xl text-white shadow-2xl">
                            <h3 class="text-xl font-black mb-3">MoveIt Servo: Ruch w Czasie Rzeczywistym</h3>
                            <p class="opacity-90 text-sm leading-relaxed mb-4">
                                Standardowe planowanie trajektorii moÅ¼e byÄ‡ zbyt wolne dla zadaÅ„ takich jak Å›ledzenie celu kamerÄ…. <strong>MoveIt Servo</strong> pozwala na przesyÅ‚anie strumieniowe komend prÄ™dkoÅ›ci bezpoÅ›rednio do manipulatorÃ³w (tzw. â€Jogging"), co umoÅ¼liwia ultra-pÅ‚ynne sterowanie joystickiem lub AI.
                            </p>
                            <div class="flex gap-3 flex-wrap">
                                <span class="px-4 py-1.5 bg-blue-700 rounded-lg text-[10px] font-black uppercase">Low Latency</span>
                                <span class="px-4 py-1.5 bg-blue-700 rounded-lg text-[10px] font-black uppercase">Reactive Control</span>
                                <span class="px-4 py-1.5 bg-blue-700 rounded-lg text-[10px] font-black uppercase">Joystick / AI Input</span>
                            </div>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 6: NVIDIA Omniverse â†’ Isaac Sim â†’ Isaac Lab
               (poÅ‚Ä…czenie: Omniverse + Isaac Sim + Isaac Lab)
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'nvidia_stack',
                title: 'Stos NVIDIA: Sim & RL',
                icon: 'ğŸŸ¢',
                links: [
                    {name: 'NVIDIA Omniverse', url: 'https://developer.nvidia.com/omniverse'},
                    {name: 'Isaac Sim Setup', url: 'https://docs.isaacsim.omniverse.nvidia.com/5.1.0/installation/index.html'},
                    {name: 'Isaac Lab GitHub', url: 'https://github.com/isaac-sim/IsaacLab'}
                ],
                description: 'Kompletny ekosystem NVIDIA dla symulacji i uczenia robotÃ³w â€“ od platformy Omniverse, przez fotorealistyczny Isaac Sim, po masowe trenowanie RL w Isaac Lab.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- CZÄ˜ÅšÄ† 1: Omniverse -->
                        <div class="nvidia-badge">NVIDIA Omniverse</div>
                        <h3 class="text-xl font-bold mb-2">Co to jest Omniverse?</h3>
                        <p class="text-stone-600 mb-4 text-sm leading-relaxed">
                            To nie jest zwykÅ‚y silnik graficzny â€“ to system operacyjny dla grafiki 3D oparty na formacie <strong>OpenUSD</strong> (Universal Scene Description). Pozwala Å‚Ä…czyÄ‡ narzÄ™dzia (Blender, Maya, CAD) w jednym czasie rzeczywistym i zarzÄ…dzaÄ‡ wersjami Isaac Sim przez centralny launcher.
                        </p>

                        <div class="grid grid-cols-2 gap-4 mb-4">
                            <div class="p-4 bg-stone-50 rounded-xl border border-stone-100">
                                <span class="block font-black text-[#76b900] text-lg">Nucleus</span>
                                <span class="text-[11px] text-stone-500">Serwer bazy danych do wspÃ³Å‚pracy live.</span>
                            </div>
                            <div class="p-4 bg-stone-50 rounded-xl border border-stone-100">
                                <span class="block font-black text-[#76b900] text-lg">Connectors</span>
                                <span class="text-[11px] text-stone-500">Wtyczki do zewnÄ™trznych aplikacji 3D.</span>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">1. Instalacja Omniverse Launcher</h3>
                            <div class="code-block">
                                <pre><code>1. WejdÅº na: nvidia.com/omniverse
2. Pobierz "Workstation Launcher"
3. Zainstaluj "Cache" oraz "Nucleus Navigator" w zakÅ‚adce Settings
4. Z Launchera zainstaluj "Isaac Sim" (zalecana wersja 2023.x+)</code></pre>
                            </div>
                        </section>

                        <hr class="section-divider">

                        <!-- CZÄ˜ÅšÄ† 2: Isaac Sim -->
                        <div class="nvidia-badge">NVIDIA Isaac Sim</div>
                        <h3 class="text-xl font-bold mb-2">Symulacja fizyczna klasy przemysÅ‚owej</h3>
                        <p class="text-stone-600 mb-4 text-sm">
                            Wykorzystuje <strong>PhysX 5</strong> do symulacji fizyki i RTX do generowania fotorealistycznych danych z sensorÃ³w (LIDAR, kamery gÅ‚Ä™bi). Dla G1 EDU to kluczowe Å›rodowisko do testowania URDF i sensorÃ³w.
                        </p>

                        <section>
                            <h3 class="text-xl font-bold mb-3">2. Integracja G1 z Isaac Sim</h3>
                            <p class="text-stone-600 mb-3">Aby robot pojawiÅ‚ siÄ™ w symulacji, zaimportuj jego model URDF i skonfiguruj mostek ROS 2.</p>
                            <div class="code-block">
                                <pre><code># WewnÄ…trz Isaac Sim:
1. Window -> Extensions -> WÅ‚Ä…cz "omni.isaac.ros2_bridge"
2. Tools -> Robotics -> URDF Importer
3. WskaÅ¼ plik G1.urdf z Unitree SDK

# WaÅ¼ne: Zaznacz "Fix Base Link" dla testÃ³w manipulacji
# lub odznacz dla testÃ³w chodu.

# Instalacja Isaac ROS (host Ubuntu):
sudo apt install ros-humble-moveit -y
sudo apt install ros-humble-isaac-ros-common -y</code></pre>
                            </div>
                        </section>

                        <div class="p-5 bg-amber-50 rounded-2xl border-l-4 border-amber-400 mb-4">
                            <h4 class="font-black text-amber-900 text-sm mb-1">PRO TIP: SYNTETYCZNE DANE</h4>
                            <p class="text-xs text-amber-800">UÅ¼yj narzÄ™dzia <strong>Replicator</strong> w Isaac Sim, aby wygenerowaÄ‡ tysiÄ…ce zdjÄ™Ä‡ z etykietami do trenowania sieci neuronowych rozpoznajÄ…cych obiekty dla Twojego G1.</p>
                        </div>

                        <div class="info-box p-5 rounded-xl">
                            <h4 class="font-bold text-green-800">ROS 2 Bridge</h4>
                            <p class="text-sm text-green-700">WÅ‚Ä…cz rozszerzenie <code>omni.isaac.ros2_bridge</code>, aby tematy ROS 2 z Twojego komputera sterowaÅ‚y robotem wewnÄ…trz symulacji.</p>
                        </div>

                        <hr class="section-divider">

                        <!-- CZÄ˜ÅšÄ† 3: Isaac Lab -->
                        <div class="nvidia-badge">NVIDIA Isaac Lab</div>
                        <h3 class="text-xl font-bold mb-2">Masowe Reinforcement Learning (dawny Orbit)</h3>
                        <p class="text-stone-600 mb-4 text-sm">
                            <strong>Isaac Lab</strong> to najwyÅ¼sza warstwa stosu NVIDIA. SÅ‚uÅ¼y do trenowania polityk chodu (locomotion) dla humanoidÃ³w G1 przy uÅ¼yciu tysiÄ™cy rÃ³wnolegÅ‚ych instancji robota na jednej karcie GPU.
                        </p>

                        <section class="grid grid-cols-1 md:grid-cols-3 gap-3 mb-4">
                            <div class="p-4 bg-stone-900 text-white rounded-xl">
                                <span class="text-[#76b900] font-black text-xs uppercase block mb-1">Parallelism</span>
                                <p class="text-[10px] opacity-70">Symuluj 4096 robotÃ³w naraz na jednej karcie RTX 4090.</p>
                            </div>
                            <div class="p-4 bg-stone-900 text-white rounded-xl">
                                <span class="text-[#76b900] font-black text-xs uppercase block mb-1">GPU Pipeline</span>
                                <p class="text-[10px] opacity-70">Dane z sensorÃ³w nie opuszczajÄ… pamiÄ™ci VRAM â€“ brak wÄ…skiego gardÅ‚a CPU.</p>
                            </div>
                            <div class="p-4 bg-stone-900 text-white rounded-xl">
                                <span class="text-[#76b900] font-black text-xs uppercase block mb-1">PPO / RSL_RL</span>
                                <p class="text-[10px] opacity-70">Wbudowane algorytmy do nauki chodu humanoida.</p>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">3. Instalacja Isaac Lab</h3>
                            <div class="code-block">
                                <pre><code># Sklonuj repozytorium Isaac Lab
git clone https://github.com/isaac-sim/IsaacLab.git
cd IsaacLab

# Skonfiguruj Å›rodowisko (wymaga zainstalowanego Isaac Sim)
./isaaclab.sh --install

# Uruchom test Å›rodowiska (G1 locomotion policy)
./isaaclab.sh -p scripts/reinforcement_learning/play.py --task Isaac-Velocity-G1-v0</code></pre>
                            </div>
                        </section>

                        <div class="p-8 bg-black rounded-3xl text-center mt-4">
                            <h3 class="text-2xl font-black text-white mb-4">Gotowy na Sim-to-Real?</h3>
                            <p class="text-stone-400 text-sm mb-6 leading-relaxed">
                                Po wytrenowaniu polityki w Isaac Lab, wyeksportuj model <code>.pt</code> (PyTorch) i wgraj go na komputer pokÅ‚adowy G1 EDU przy uÅ¼yciu Unitree SDK2.
                            </p>
                            <div class="flex justify-center gap-4">
                                <a href="https://docs.isaacsim.omniverse.nvidia.com/" target="_blank" class="px-6 py-2 border border-[#76b900] text-[#76b900] rounded-full text-xs font-black uppercase hover:bg-[#76b900] hover:text-black transition-all">Sim2Real Docs â†—</a>
                                <a href="https://github.com/unitreerobotics/unitree_sdk2" target="_blank" class="px-6 py-2 bg-[#76b900] text-black rounded-full text-xs font-black uppercase hover:bg-green-400 transition-all">Pobierz G1 Asset â†—</a>
                            </div>
                        </div>

                    </div>
                `
            },

            /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 7: Wymagania sprzÄ™towe i Deploy
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'hardware',
                title: 'Wymagania i Deploy',
                icon: 'ğŸ–¥ï¸',
                links: [{name: 'Isaac Sim Docs', url: 'https://docs.isaacsim.omniverse.nvidia.com/'}],
                description: 'Zasoby sprzÄ™towe potrzebne do uruchomienia peÅ‚nego stosu NVIDIA i wdroÅ¼enia modeli na robot.',
                details: `
                    <div class="space-y-6 step-content">
                        <table class="w-full text-sm border-collapse">
                            <thead>
                                <tr class="border-b-2 border-stone-200">
                                    <th class="text-left py-3 font-black">Komponent</th>
                                    <th class="text-left py-3 font-black">Minimum</th>
                                    <th class="text-left py-3 font-black">Zalecane (G1 Dev)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr class="border-b border-stone-100">
                                    <td class="py-3 font-bold">GPU</td>
                                    <td class="py-3">RTX 3070 (8 GB)</td>
                                    <td class="py-3 text-[#76b900] font-bold">RTX 4090 (24 GB)</td>
                                </tr>
                                <tr class="border-b border-stone-100">
                                    <td class="py-3 font-bold">System</td>
                                    <td class="py-3">Ubuntu 20.04</td>
                                    <td class="py-3">Ubuntu 22.04 LTS</td>
                                </tr>
                                <tr class="border-b border-stone-100">
                                    <td class="py-3 font-bold">Sterowniki NVIDIA</td>
                                    <td class="py-3">525.x</td>
                                    <td class="py-3">535.x lub nowsze</td>
                                </tr>
                                <tr class="border-b border-stone-100">
                                    <td class="py-3 font-bold">RAM</td>
                                    <td class="py-3">32 GB</td>
                                    <td class="py-3">64 GB DDR5</td>
                                </tr>
                                <tr>
                                    <td class="py-3 font-bold">Dysk</td>
                                    <td class="py-3">SSD 500 GB</td>
                                    <td class="py-3">NVMe 2 TB</td>
                                </tr>
                            </tbody>
                        </table>

                        <div class="p-6 bg-amber-600 rounded-2xl text-white shadow-xl mt-4">
                            <h4 class="font-bold mb-2">Wszystko gotowe!</h4>
                            <p class="text-sm opacity-90">PeÅ‚ny stos jest skonfigurowany. MoÅ¼esz uruchomiÄ‡ skrypt testowy:</p>
                            <code class="block mt-4 bg-amber-900/50 p-2 rounded text-xs">python3 ~/unitree_sdk2/example/helloworld.py</code>
                        </div>
                    </div>
                `
            },

            /* â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
               MODUÅ 8: AUTONOMICZNA MISJA â€” Nawigacja, Manipulacja,
               Detekcja CzÅ‚owieka, Gesty i Mowa
            â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• */
            {
                id: 'autonomous_mission',
                title: 'Autonomiczna Misja G1',
                icon: 'ğŸ§­',
                links: [
                    {name: 'Nav2 Docs', url: 'https://docs.nav2.org/'},
                    {name: 'slam_toolbox', url: 'https://github.com/SteveMacenski/slam_toolbox'},
                    {name: 'YOLOv8', url: 'https://docs.ultralytics.com/'},
                    {name: 'Coqui TTS', url: 'https://github.com/coqui-ai/TTS'}
                ],
                description: 'Kompletny stos narzÄ™dzi do realizacji misji autonomicznej: nawigacja SLAM, planowanie chwytÃ³w, detekcja czÅ‚owieka, synteza mowy i komunikacja przez gesty.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- â”€â”€â”€ WIZJA MISJI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <div class="bg-stone-950 rounded-2xl p-7 text-white">
                            <p class="text-amber-400 text-[10px] font-black uppercase tracking-widest mb-3">Scenariusz misji</p>
                            <h3 class="text-2xl font-black mb-4 leading-tight">G1 wchodzi do pokoju, zbiera elementy,<br>wita czÅ‚owieka i skÅ‚ada mu raport.</h3>
                            <div class="grid grid-cols-2 md:grid-cols-5 gap-2 mt-5">
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-2xl mb-1">ğŸ—ºï¸</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">1. Nawigacja</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">SLAM + Nav2</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-2xl mb-1">ğŸ“¦</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">2. Detekcja</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">YOLOv8 + PCL</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-2xl mb-1">ğŸ¦¾</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">3. Chwyt</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">MoveIt + GraspNet</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-2xl mb-1">ğŸ‘¤</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">4. CzÅ‚owiek</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">DeepFace + MP</p>
                                </div>
                                <div class="p-3 bg-stone-800 rounded-xl text-center">
                                    <div class="text-2xl mb-1">ğŸ‘‹</div>
                                    <p class="text-[9px] font-bold uppercase text-stone-400">5. Powitanie</p>
                                    <p class="text-[9px] text-stone-500 mt-0.5">TTS + Gesty</p>
                                </div>
                            </div>
                        </div>

                        <!-- â”€â”€â”€ ETAP 1: NAWIGACJA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ—ºï¸ Etap 1 â€” Autonomiczna Nawigacja (SLAM + Nav2)</div>

                        <p class="text-stone-600 leading-relaxed">
                            Zanim G1 zacznie cokolwiek robiÄ‡, musi wiedzieÄ‡, gdzie jest i jak siÄ™ poruszaÄ‡ bezpiecznie. UÅ¼ywamy dwÃ³ch warstw: <strong>SLAM</strong> (jednoczesne mapowanie i lokalizacja) buduje mapÄ™ pomieszczenia, a <strong>Nav2</strong> planuje i wykonuje trasy z unikaniem przeszkÃ³d.
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-2 gap-5">
                            <div class="p-5 rounded-2xl border border-stone-100 bg-white shadow-sm">
                                <span class="block font-black text-sm text-stone-800 mb-1">slam_toolbox</span>
                                <p class="text-xs text-stone-500 leading-relaxed">Buduje mapÄ™ 2D z danych LiDAR lub symulowanego skanu z kamery gÅ‚Ä™bi (D435i). Pracuje online (podczas ruchu) lub offline (z bagfile). Persystuje mapÄ™ miÄ™dzy sesjami.</p>
                            </div>
                            <div class="p-5 rounded-2xl border border-stone-100 bg-white shadow-sm">
                                <span class="block font-black text-sm text-stone-800 mb-1">Nav2 (Navigation2)</span>
                                <p class="text-xs text-stone-500 leading-relaxed">Planuje globalne i lokalne trasy, zarzÄ…dza costmapami, realizuje akcje â€przejdÅº do punktu XY" i â€obrÃ³Ä‡ siÄ™ do kÄ…ta Z". ObsÅ‚uguje recovery behaviors gdy robot utknie.</p>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja stosu nawigacyjnego</h3>
                            <div class="code-block">
                                <pre><code># Nav2 + SLAM Toolbox + lokalizacja AMCL
sudo apt install ros-humble-nav2-bringup \
                 ros-humble-slam-toolbox \
                 ros-humble-nav2-map-server \
                 ros-humble-navigation2 -y

# Konwerter chmury punktÃ³w â†’ scan (dla kamer gÅ‚Ä™bi bez LiDARu)
sudo apt install ros-humble-depthimage-to-laserscan -y</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Faza 1: Budowanie mapy (SLAM online)</h3>
                            <div class="code-block">
                                <pre><code># Terminal 1: Uruchom mostek ROS2 dla G1
ros2 launch unitree_ros2 g1_bridge.launch.py interface:=eth0

# Terminal 2: Uruchom SLAM Toolbox w trybie online
ros2 launch slam_toolbox online_async_launch.py \
    slam_params_file:=./config/slam_params.yaml \
    use_sim_time:=false

# Terminal 3: Teleoperacja do eksploracji pomieszczenia
ros2 run teleop_twist_keyboard teleop_twist_keyboard \
    --ros-args --remap /cmd_vel:=/cmd_vel

# Terminal 4: Wizualizacja w RViz2
ros2 launch nav2_bringup rviz_launch.py

# Po eksploracji â€“ zapisz mapÄ™
ros2 run nav2_map_server map_saver_cli -f ~/maps/room_map</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Faza 2: Autonomiczna nawigacja po mapie</h3>
                            <div class="code-block">
                                <pre><code># Plik: navigate_to_box.py
import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from nav2_msgs.action import NavigateToPose
from geometry_msgs.msg import PoseStamped
import tf_transformations

class G1Navigator(Node):
    def __init__(self):
        super().__init__('g1_navigator')
        self._client = ActionClient(self, NavigateToPose, 'navigate_to_pose')

    def go_to(self, x: float, y: float, yaw_deg: float = 0.0):
        goal = NavigateToPose.Goal()
        pose = PoseStamped()
        pose.header.frame_id = 'map'
        pose.header.stamp = self.get_clock().now().to_msg()
        pose.pose.position.x = x
        pose.pose.position.y = y

        # Konwersja kÄ…ta yaw â†’ quaternion
        q = tf_transformations.quaternion_from_euler(0, 0, yaw_deg * 3.14159 / 180)
        pose.pose.orientation.z = q[2]
        pose.pose.orientation.w = q[3]

        goal.pose = pose
        self.get_logger().info(f'NawigujÄ™ do: x={x}, y={y}, yaw={yaw_deg}Â°')
        self._client.wait_for_server()
        future = self._client.send_goal_async(goal)
        return future

def main():
    rclpy.init()
    nav = G1Navigator()

    # 1. IdÅº do lokalizacji pudeÅ‚ka
    nav.go_to(x=2.5, y=1.0, yaw_deg=90.0)
    rclpy.spin_once(nav, timeout_sec=30.0)

    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <div class="pro-tip p-5 rounded-xl">
                            <h4 class="font-bold text-blue-800">Costmapy i strefy zakazane</h4>
                            <p class="text-sm text-blue-700">Nav2 uÅ¼ywa dwÃ³ch costmap: <strong>globalnej</strong> (plan caÅ‚ej trasy na statycznej mapie) i <strong>lokalnej</strong> (unikanie dynamicznych przeszkÃ³d w czasie rzeczywistym, np. przechodzÄ…cej osoby). Skonfiguruj <code>inflation_radius</code> na co najmniej 0.35m dla humanoidÃ³w, aby nogi robota nie zachodziÅ‚y na przeszkody.</p>
                        </div>

                        <!-- â”€â”€â”€ ETAP 2: DETEKCJA OBIEKTÃ“W â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ“¦ Etap 2 â€” Detekcja i Lokalizacja ObiektÃ³w (YOLOv8 + PCL)</div>

                        <p class="text-stone-600 leading-relaxed">
                            Aby chwyÄ‡ pudeÅ‚ko, robot musi je najpierw <strong>wykryÄ‡</strong> na obrazie 2D (YOLOv8), a nastÄ™pnie ustaliÄ‡ jego dokÅ‚adnÄ… <strong>pozycjÄ™ 3D</strong> w przestrzeni za pomocÄ… chmury punktÃ³w z kamery gÅ‚Ä™bi (Intel RealSense D435i lub ZED2).
                        </p>

                        <div class="overflow-x-auto rounded-xl border border-stone-100 mb-4">
                            <table class="w-full text-xs border-collapse">
                                <thead>
                                    <tr class="bg-stone-900 text-amber-400">
                                        <th class="text-left px-4 py-3 font-black">NarzÄ™dzie</th>
                                        <th class="text-left px-4 py-3 font-black">Zadanie</th>
                                        <th class="text-left px-4 py-3 font-black">WyjÅ›cie</th>
                                    </tr>
                                </thead>
                                <tbody class="divide-y divide-stone-100 bg-white">
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-mono font-bold text-blue-600">YOLOv8</td>
                                        <td class="px-4 py-3 text-stone-600">Detekcja bounding box obiektu na obrazie RGB</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">bbox [x1,y1,x2,y2] + klasa + pewnoÅ›Ä‡</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-mono font-bold text-blue-600">RealSense D435i</td>
                                        <td class="px-4 py-3 text-stone-600">Kamera gÅ‚Ä™bi â€“ surowa chmura punktÃ³w XYZ</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">sensor_msgs/PointCloud2</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-mono font-bold text-blue-600">PCL (Point Cloud Lib)</td>
                                        <td class="px-4 py-3 text-stone-600">Wycinanie ROI, filtracja, centroid 3D obiektu</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">geometry_msgs/Point (x, y, z)</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-mono font-bold text-blue-600">GraspNet / AnyGrasp</td>
                                        <td class="px-4 py-3 text-stone-600">Przewidywanie optymalnej pozy chwytu z chmury punktÃ³w</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">6-DOF grasp pose [x,y,z,roll,pitch,yaw]</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Instalacja i konfiguracja YOLOv8</h3>
                            <div class="code-block">
                                <pre><code># Instalacja Ultralytics YOLOv8
pip install ultralytics opencv-python pyrealsense2

# Instalacja ROS2 wrapper dla RealSense
sudo apt install ros-humble-realsense2-camera -y

# Uruchomienie kamery gÅ‚Ä™bi
ros2 launch realsense2_camera rs_launch.py \
    enable_color:=true \
    enable_depth:=true \
    align_depth.enable:=true</code></pre>
                            </div>
                        </section>

                        <section>
                            <h3 class="text-xl font-bold mb-3">WÄ™zeÅ‚ detekcji: YOLOv8 + centroid 3D z chmury punktÃ³w</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, PointCloud2
from geometry_msgs.msg import PointStamped
from cv_bridge import CvBridge
import sensor_msgs_py.point_cloud2 as pc2
from ultralytics import YOLO
import numpy as np

class BoxDetector(Node):
    def __init__(self):
        super().__init__('box_detector')
        self.model = YOLO('yolov8m.pt')          # model medium
        self.bridge = CvBridge()
        self.latest_cloud = None

        # Subskrybuj obraz RGB i chmurÄ™ punktÃ³w
        self.create_subscription(Image,
            '/camera/color/image_raw', self.img_cb, 10)
        self.create_subscription(PointCloud2,
            '/camera/depth/color/points', self.cloud_cb, 10)

        # Publikuj wykrytÄ… pozycjÄ™ pudeÅ‚ka
        self.pos_pub = self.create_publisher(
            PointStamped, '/detected_box_position', 10)

    def cloud_cb(self, msg):
        self.latest_cloud = msg

    def img_cb(self, msg):
        if self.latest_cloud is None:
            return

        frame = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
        results = self.model(frame, classes=[73])  # 73 = 'box' w COCO

        for det in results[0].boxes:
            conf = float(det.conf)
            if conf < 0.6:
                continue

            # Åšrodek bounding boxa w pikselach
            x1, y1, x2, y2 = map(int, det.xyxy[0])
            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2

            # Pobierz punkt 3D z chmury w miejscu (cx, cy)
            # Iteruj po punktach z wycinka ROI
            roi_points = []
            for pt in pc2.read_points(self.latest_cloud,
                                       field_names=('x','y','z'),
                                       skip_nans=True,
                                       uvs=[(cx, cy)]):
                roi_points.append(pt)

            if not roi_points:
                continue

            xyz = np.array(roi_points).mean(axis=0)

            out = PointStamped()
            out.header = self.latest_cloud.header
            out.point.x, out.point.y, out.point.z = \
                float(xyz[0]), float(xyz[1]), float(xyz[2])
            self.pos_pub.publish(out)

            self.get_logger().info(
                f'PudeÅ‚ko @ x={xyz[0]:.3f} y={xyz[1]:.3f} z={xyz[2]:.3f}m '
                f'(pewnoÅ›Ä‡: {conf:.0%})')

def main():
    rclpy.init()
    rclpy.spin(BoxDetector())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ ETAP 3: CHWYT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ¦¾ Etap 3 â€” Planowanie i Wykonanie Chwytu</div>

                        <p class="text-stone-600 leading-relaxed">
                            MajÄ…c pozycjÄ™ 3D pudeÅ‚ka, robot musi wygenerowaÄ‡ optymalnÄ… pozÄ™ chwytu i wykonaÄ‡ ruch ramieniem bez kolizji. Pipeline Å‚Ä…czy <strong>GraspNet</strong> (predykcja pozy) z <strong>MoveIt 2</strong> (planowanie trajektorii).
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
                            <div class="p-4 bg-stone-50 rounded-xl border border-stone-100 text-center">
                                <div class="text-2xl mb-2">ğŸ“</div>
                                <span class="text-xs font-black uppercase text-stone-700 block mb-1">Pre-grasp pose</span>
                                <p class="text-[10px] text-stone-500">RamiÄ™ 15 cm nad obiektem, palce otwarte â€“ bezpieczna pozycja wyjÅ›ciowa przed chwytem.</p>
                            </div>
                            <div class="p-4 bg-stone-50 rounded-xl border border-stone-100 text-center">
                                <div class="text-2xl mb-2">ğŸ«³</div>
                                <span class="text-xs font-black uppercase text-stone-700 block mb-1">Grasp pose</span>
                                <p class="text-[10px] text-stone-500">Ruch liniowy w dÃ³Å‚ (CartesianPath), zaciÅ›niÄ™cie chwytaka, weryfikacja siÅ‚y dotyku.</p>
                            </div>
                            <div class="p-4 bg-stone-50 rounded-xl border border-stone-100 text-center">
                                <div class="text-2xl mb-2">ğŸ </div>
                                <span class="text-xs font-black uppercase text-stone-700 block mb-1">Retreat pose</span>
                                <p class="text-[10px] text-stone-500">Uniesienie ramienia z obiektem, powrÃ³t do bezpiecznej pozycji transportowej.</p>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Pipeline chwytu: MoveIt 2 (Python)</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
# Skrypt wykonuje sekwencjÄ™: pre-grasp â†’ grasp â†’ retreat
import rclpy
from rclpy.node import Node
from moveit.planning import MoveItPy
from geometry_msgs.msg import Pose
import numpy as np

class GraspExecutor(Node):
    def __init__(self):
        super().__init__('grasp_executor')
        self.robot = MoveItPy(node_name='grasp_moveit')
        self.arm = self.robot.get_planning_component('right_arm')
        self.gripper = self.robot.get_planning_component('right_gripper')

    def open_gripper(self):
        self.gripper.set_goal_state(configuration_name='open')
        self.gripper.plan_and_execute()

    def close_gripper(self):
        self.gripper.set_goal_state(configuration_name='closed')
        self.gripper.plan_and_execute()

    def move_to_pose(self, x, y, z, roll=0.0, pitch=1.57, yaw=0.0):
        pose = Pose()
        pose.position.x = x
        pose.position.y = y
        pose.position.z = z
        # Chwytanie pionowe (pitch = Ï€/2)
        from scipy.spatial.transform import Rotation as R
        q = R.from_euler('xyz', [roll, pitch, yaw]).as_quat()
        pose.orientation.x = q[0]
        pose.orientation.y = q[1]
        pose.orientation.z = q[2]
        pose.orientation.w = q[3]

        self.arm.set_goal_state(pose_stamped_msg=pose,
                                 pose_link='right_hand_link')
        result = self.arm.plan_and_execute()
        return result.status.status == 4  # SUCCESS

    def grasp_object(self, obj_x, obj_y, obj_z):
        self.get_logger().info('--- START SEKWENCJI CHWYTU ---')

        # 1. OtwÃ³rz chwytak
        self.open_gripper()

        # 2. Pozycja pre-grasp (15 cm nad obiektem)
        self.get_logger().info('Pre-grasp...')
        ok = self.move_to_pose(obj_x, obj_y, obj_z + 0.15)
        if not ok:
            self.get_logger().error('Pre-grasp FAILED')
            return False

        # 3. OpuÅ›Ä‡ na obiekt (ruch kartezjaÅ„ski)
        self.get_logger().info('Grasp...')
        ok = self.move_to_pose(obj_x, obj_y, obj_z + 0.02)
        if not ok:
            self.get_logger().error('Grasp FAILED')
            return False

        # 4. Zamknij chwytak
        self.close_gripper()

        # 5. Retreat â€“ unieÅ› z obiektem
        self.get_logger().info('Retreat...')
        self.move_to_pose(obj_x, obj_y, obj_z + 0.25)

        self.get_logger().info('âœ… Chwyt zakoÅ„czony sukcesem!')
        return True</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ ETAP 4: DETEKCJA CZÅOWIEKA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ‘¤ Etap 4 â€” Detekcja i Identyfikacja CzÅ‚owieka</div>

                        <p class="text-stone-600 leading-relaxed">
                            Po wykonaniu zadania robot musi znaleÅºÄ‡ czÅ‚owieka w pomieszczeniu. Stosujemy warstwowy pipeline: YOLOv8 wykrywa sylwetkÄ™ (szybkie), MediaPipe analizuje postawÄ™ ciaÅ‚a, a DeepFace rozpoznaje twarz i odczytuje nastrÃ³j.
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-3 gap-4 mb-4">
                            <div class="p-5 rounded-xl border-l-4 border-amber-400 bg-amber-50">
                                <span class="text-xs font-black uppercase text-amber-700 block mb-1">YOLOv8-pose</span>
                                <h4 class="font-bold text-sm mb-1">Szybka detekcja sylwetki</h4>
                                <p class="text-xs text-stone-500">Wykrywa ludzi i 17 punktÃ³w szkieletu w &lt;20ms. Daje kierunek i odlegÅ‚oÅ›Ä‡ osoby od kamery.</p>
                            </div>
                            <div class="p-5 rounded-xl border-l-4 border-blue-400 bg-blue-50">
                                <span class="text-xs font-black uppercase text-blue-700 block mb-1">MediaPipe Holistic</span>
                                <h4 class="font-bold text-sm mb-1">PeÅ‚na analiza ciaÅ‚a</h4>
                                <p class="text-xs text-stone-500">33 punkty postawy + 21 punktÃ³w kaÅ¼dej dÅ‚oni + 468 punktÃ³w twarzy. Podstawa dla interpretacji gestÃ³w.</p>
                            </div>
                            <div class="p-5 rounded-xl border-l-4 border-rose-400 bg-rose-50">
                                <span class="text-xs font-black uppercase text-rose-700 block mb-1">DeepFace</span>
                                <h4 class="font-bold text-sm mb-1">Identyfikacja i emocje</h4>
                                <p class="text-xs text-stone-500">Rozpoznaje toÅ¼samoÅ›Ä‡ z bazy danych, zwraca nastrÃ³j (happy, sad, neutralâ€¦) i wiek rozmÃ³wcy.</p>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">WÄ™zeÅ‚ detekcji czÅ‚owieka: YOLOv8 + DeepFace</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from std_msgs.msg import String
from cv_bridge import CvBridge
from ultralytics import YOLO
from deepface import DeepFace
import cv2, json

class HumanDetector(Node):
    def __init__(self):
        super().__init__('human_detector')
        self.yolo = YOLO('yolov8n-pose.pt')  # model nano-pose
        self.bridge = CvBridge()
        self.human_found = False

        self.create_subscription(Image,
            '/camera/color/image_raw', self.cb, 10)

        # Temat z informacjami o czÅ‚owieku
        self.pub = self.create_publisher(String, '/human_info', 10)
        self.get_logger().info('Szukam czÅ‚owieka...')

    def cb(self, msg):
        frame = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
        results = self.yolo(frame, classes=[0])  # 0 = 'person'

        persons = results[0].boxes
        if len(persons) == 0:
            return

        # WeÅº osobÄ™ z najwyÅ¼szÄ… pewnoÅ›ciÄ…
        best = max(persons, key=lambda b: float(b.conf))
        x1, y1, x2, y2 = map(int, best.xyxy[0])
        face_roi = frame[y1:y1+int((y2-y1)*0.35), x1:x2]  # gÃ³rne 35% = twarz

        info = {
            'position_px': [int((x1+x2)/2), int((y1+y2)/2)],
            'confidence':  float(best.conf),
            'emotion':     'unknown',
            'age':         'unknown',
            'identity':    'unknown'
        }

        # Analiza twarzy (moÅ¼e byÄ‡ wolna â€“ uruchamiaj co N klatek)
        try:
            if face_roi.size > 0:
                analysis = DeepFace.analyze(
                    face_roi,
                    actions=['emotion', 'age'],
                    enforce_detection=False,
                    silent=True
                )
                info['emotion'] = analysis[0]['dominant_emotion']
                info['age']     = int(analysis[0]['age'])
        except Exception:
            pass

        out = String()
        out.data = json.dumps(info, ensure_ascii=False)
        self.pub.publish(out)

        self.get_logger().info(
            f"CzÅ‚owiek wykryty | nastrÃ³j: {info['emotion']} "
            f"| wiek: {info['age']} | pewnoÅ›Ä‡: {info['confidence']:.0%}")

def main():
    rclpy.init()
    rclpy.spin(HumanDetector())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ ETAP 5: GESTY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ‘‹ Etap 5 â€” Powitanie: Gesty Ramieniem</div>

                        <p class="text-stone-600 leading-relaxed">
                            Robot wykonuje sekwencjÄ™ gestÃ³w ramieniem przez SDK2 Low-Level â€” animowane, naturalne powitanie skÅ‚ada siÄ™ z podniesienia ramienia, machania i powrotu do pozycji neutralnej.
                        </p>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Gest machania â€” interpolacja sinusoidalna (Python SDK2)</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
# Gest powitania: podniesienie ramienia + machanie 3x
from unitree_sdk2.common import ChannelFactory, ChannelPublisher, ChannelSubscriber
from unitree_sdk2.hg.h1_2_arm_sdk import LowCmd, LowState
import time, math

ChannelFactory.Initialize(0, "eth0")

latest_state = [None]
def on_state(msg): latest_state[0] = msg

sub = ChannelSubscriber("rt/lowstate", LowState)
sub.InitSubscriber(on_state)
pub = ChannelPublisher("rt/lowcmd", LowCmd)
pub.InitPublisher()

while latest_state[0] is None:
    time.sleep(0.01)

# â”€â”€ Indeksy stawÃ³w prawego ramienia G1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (sprawdÅº dokumentacjÄ™ SDK2 dla konkretnych indeksÃ³w G1)
SHOULDER_PITCH = 13   # prawe ramiÄ™ â€“ uniesienie
SHOULDER_ROLL  = 14   # prawe ramiÄ™ â€“ odwiedzenie
ELBOW          = 15   # prawy Å‚okieÄ‡

Kp = 60.0
Kd = 2.0
FREQ = 200  # Hz
dt   = 1.0 / FREQ

def send_cmd(targets: dict):
    """targets: {joint_index: target_angle_rad}"""
    cmd = LowCmd()
    state = latest_state[0]
    for i in range(len(cmd.motor_cmd)):
        cmd.motor_cmd[i].mode = 0x01
        cmd.motor_cmd[i].q   = state.motor_state[i].q
        cmd.motor_cmd[i].kp  = 0.0
        cmd.motor_cmd[i].kd  = 3.0
        cmd.motor_cmd[i].tau = 0.0
    for idx, angle in targets.items():
        cmd.motor_cmd[idx].q  = angle
        cmd.motor_cmd[idx].kp = Kp
        cmd.motor_cmd[idx].kd = Kd
    pub.Write(cmd)

def interpolate(start, end, duration):
    """PÅ‚ynne przejÅ›cie z interpolacjÄ… cosinus (ease-in/out)"""
    steps = int(duration * FREQ)
    for i in range(steps):
        t = i / steps
        alpha = (1 - math.cos(math.pi * t)) / 2
        current = {j: start[j] + alpha * (end[j] - start[j]) for j in start}
        send_cmd(current)
        time.sleep(dt)

print("â–¶ Gest powitania: start")

NEUTRAL  = {SHOULDER_PITCH: 0.0,  SHOULDER_ROLL: -0.3, ELBOW: 0.5}
RAISED   = {SHOULDER_PITCH: -1.2, SHOULDER_ROLL: -0.6, ELBOW: 0.8}
WAVE_A   = {SHOULDER_PITCH: -1.2, SHOULDER_ROLL: -0.3, ELBOW: 1.2}
WAVE_B   = {SHOULDER_PITCH: -1.2, SHOULDER_ROLL: -0.9, ELBOW: 0.4}

# 1. UnieÅ› ramiÄ™
interpolate(NEUTRAL, RAISED, duration=0.8)

# 2. Macha 3 razy
for _ in range(3):
    interpolate(RAISED, WAVE_A, duration=0.35)
    interpolate(WAVE_A, WAVE_B, duration=0.35)

# 3. PowrÃ³t do neutralnej
interpolate(RAISED, NEUTRAL, duration=1.0)

print("âœ… Gest zakoÅ„czony")</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ ETAP 5b: MOWA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ”Š Etap 5b â€” Synteza Mowy (TTS)</div>

                        <p class="text-stone-600 leading-relaxed">
                            RÃ³wnolegle z gestem robot mÃ³wi do czÅ‚owieka. Stosujemy <strong>Coqui TTS</strong> do polskiej, naturalnie brzmiÄ…cej syntezy offline lub <strong>pyttsx3</strong> jako lekkÄ… alternatywÄ™ bez GPU.
                        </p>

                        <div class="grid grid-cols-1 md:grid-cols-2 gap-5 mb-4">
                            <div class="p-5 bg-white rounded-xl border border-stone-100 shadow-sm">
                                <span class="text-xs font-black uppercase text-stone-600 block mb-2">Coqui TTS</span>
                                <p class="text-xs text-stone-500 mb-3">Offline, wysokiej jakoÅ›ci synteza z modelem VITS lub XTTS v2. ObsÅ‚uguje jÄ™zyk polski. Wymaga GPU dla szybkoÅ›ci real-time.</p>
                                <div class="code-block" style="margin:0; font-size:0.75rem; padding: 0.75rem;">
                                    <pre><code>pip install TTS
# Listowanie modeli PL
tts --list_models | grep pl</code></pre>
                                </div>
                            </div>
                            <div class="p-5 bg-white rounded-xl border border-stone-100 shadow-sm">
                                <span class="text-xs font-black uppercase text-stone-600 block mb-2">pyttsx3</span>
                                <p class="text-xs text-stone-500 mb-3">Lekka biblioteka TTS offline, dziaÅ‚a na CPU. Idealna do prostych komunikatÃ³w bez opÃ³ÅºnieÅ„ instalacji modeli.</p>
                                <div class="code-block" style="margin:0; font-size:0.75rem; padding: 0.75rem;">
                                    <pre><code>pip install pyttsx3
# DziaÅ‚a bez Internetu
# GÅ‚os zaleÅ¼y od espeak-ng</code></pre>
                                </div>
                            </div>
                        </div>

                        <section>
                            <h3 class="text-xl font-bold mb-3">WÄ™zeÅ‚ mowy: dynamiczny komunikat na podstawie wykrytego nastroju</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from TTS.api import TTS
import sounddevice as sd
import json, numpy as np, threading

class G1SpeechNode(Node):
    GREETINGS = {
        'happy':   'CzeÅ›Ä‡! WidzÄ™, Å¼e jesteÅ› w Å›wietnym nastroju! '
                   'WÅ‚aÅ›nie skoÅ„czyÅ‚em pakowanie elementÃ³w. Jak mogÄ™ pomÃ³c?',
        'sad':     'Hej. ZauwaÅ¼yÅ‚em, Å¼e moÅ¼e nie masz najlepszego dnia. '
                   'SkoÅ„czyÅ‚em zadanie â€“ mam nadziejÄ™, Å¼e to CiÄ™ ucieszy!',
        'neutral': 'CzeÅ›Ä‡! Jestem G1. WykonaÅ‚em powierzone mi zadanie. '
                   'PudeÅ‚ko jest spakowane i gotowe.',
        'angry':   'DzieÅ„ dobry. Staram siÄ™ nie przeszkadzaÄ‡. '
                   'Zadanie zostaÅ‚o wykonane pomyÅ›lnie.',
        'unknown': 'Witaj! Jestem robot Unitree G1. '
                   'SkoÅ„czyÅ‚em pracÄ™ i chciaÅ‚em CiÄ™ przywitaÄ‡!'
    }

    def __init__(self):
        super().__init__('g1_speech')
        # Wczytaj polski model TTS (VITS multilingual)
        self.tts = TTS('tts_models/multilingual/multi-dataset/xtts_v2',
                       gpu=True)
        self.spoken = set()

        self.create_subscription(String, '/human_info', self.on_human, 10)
        self.get_logger().info('WÄ™zeÅ‚ mowy gotowy.')

    def on_human(self, msg):
        info = json.loads(msg.data)
        emotion = info.get('emotion', 'unknown')

        # MÃ³w tylko raz per sesjÄ™ spotkania
        if emotion in self.spoken:
            return
        self.spoken.add(emotion)

        text = self.GREETINGS.get(emotion, self.GREETINGS['unknown'])
        self.get_logger().info(f'MÃ³wiÄ™ ({emotion}): {text[:50]}...')

        # Synteza w osobnym wÄ…tku (nie blokuj pÄ™tli ROS2)
        threading.Thread(target=self._speak, args=(text,), daemon=True).start()

    def _speak(self, text: str):
        wav = self.tts.tts(text=text, language='pl',
                           speaker_wav='/home/g1/voice_ref.wav')
        audio = np.array(wav, dtype=np.float32)
        sd.play(audio, samplerate=22050)
        sd.wait()

def main():
    rclpy.init()
    rclpy.spin(G1SpeechNode())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ ORCHESTRATOR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ¼ Orkiestrator Misji â€” Spinanie wszystkiego razem</div>

                        <p class="text-stone-600 leading-relaxed">
                            Wszystkie etapy Å‚Ä…czy wÄ™zeÅ‚-orkiestrator oparty na <strong>maszynie stanÃ³w</strong> (Behavior Tree lub prosty FSM). Decyduje, ktÃ³ry krok jest aktywny i przekazuje dane miÄ™dzy moduÅ‚ami.
                        </p>

                        <section>
                            <h3 class="text-xl font-bold mb-3">Prosta Maszyna StanÃ³w (FSM) misji</h3>
                            <div class="code-block">
                                <pre><code">#!/usr/bin/env python3
# Orkiestrator misji G1 â€“ Finite State Machine
from enum import Enum, auto
import rclpy
from rclpy.node import Node
from std_msgs.msg import String
from geometry_msgs.msg import PointStamped
import json

class State(Enum):
    NAVIGATE_TO_AREA  = auto()  # jedÅº do strefy pracy
    DETECT_OBJECTS    = auto()  # szukaj elementÃ³w i pudeÅ‚ka
    GRASP_ITEMS       = auto()  # podnieÅ› elementy, wÅ‚Ã³Å¼ do pudeÅ‚ka
    FIND_HUMAN        = auto()  # nawiguj do czÅ‚owieka
    GREET             = auto()  # gest + mowa
    DONE              = auto()  # zakoÅ„cz misjÄ™

class MissionFSM(Node):
    def __init__(self):
        super().__init__('mission_fsm')
        self.state = State.NAVIGATE_TO_AREA
        self.box_pos = None
        self.human_info = None

        # Subskrybuj wyniki moduÅ‚Ã³w
        self.create_subscription(PointStamped,
            '/detected_box_position', self._on_box, 10)
        self.create_subscription(String,
            '/human_info', self._on_human, 10)

        # Komendy do moduÅ‚Ã³w
        self.nav_pub   = self.create_publisher(String, '/mission/nav_cmd', 10)
        self.grasp_pub = self.create_publisher(String, '/mission/grasp_cmd', 10)
        self.greet_pub = self.create_publisher(String, '/mission/greet_cmd', 10)

        # PÄ™tla FSM co 500ms
        self.create_timer(0.5, self.tick)
        self.get_logger().info('ğŸš€ Misja G1 rozpoczÄ™ta!')

    def _on_box(self, msg):
        self.box_pos = msg

    def _on_human(self, msg):
        self.human_info = json.loads(msg.data)

    def tick(self):
        log = self.get_logger()

        if self.state == State.NAVIGATE_TO_AREA:
            log.info('[FSM] â†’ NawigujÄ™ do strefy pracy...')
            self.nav_pub.publish(String(data='goto:2.5,1.0,90'))
            # W rzeczywistoÅ›ci czekasz na sukces akcji nav2
            self.state = State.DETECT_OBJECTS

        elif self.state == State.DETECT_OBJECTS:
            if self.box_pos is not None:
                log.info('[FSM] â†’ PudeÅ‚ko wykryte! PrzechodzÄ™ do chwytu.')
                self.state = State.GRASP_ITEMS
            else:
                log.info('[FSM] Szukam obiektÃ³w...')

        elif self.state == State.GRASP_ITEMS:
            p = self.box_pos.point
            cmd = f'grasp:{p.x:.3f},{p.y:.3f},{p.z:.3f}'
            self.grasp_pub.publish(String(data=cmd))
            log.info(f'[FSM] â†’ Chwyt @ ({p.x:.2f}, {p.y:.2f}, {p.z:.2f})')
            self.state = State.FIND_HUMAN

        elif self.state == State.FIND_HUMAN:
            if self.human_info is not None:
                log.info('[FSM] â†’ CzÅ‚owiek wykryty! PodchodzÄ™ do powitania.')
                emotion = self.human_info.get('emotion', 'unknown')
                pos = self.human_info.get('position_px', [0, 0])
                self.nav_pub.publish(String(data='goto:human'))
                self.state = State.GREET
            else:
                log.info('[FSM] Szukam czÅ‚owieka w pomieszczeniu...')

        elif self.state == State.GREET:
            emotion = self.human_info.get('emotion', 'unknown')
            self.greet_pub.publish(String(data=f'greet:{emotion}'))
            log.info(f'[FSM] â†’ Pozdrawiam! NastrÃ³j rozmÃ³wcy: {emotion}')
            self.state = State.DONE

        elif self.state == State.DONE:
            log.info('[FSM] âœ… Misja zakoÅ„czona pomyÅ›lnie!')
            self.destroy_timer(self.timers[0] if self.timers else None)

def main():
    rclpy.init()
    rclpy.spin(MissionFSM())
    rclpy.shutdown()

if __name__ == '__main__':
    main()</code></pre>
                            </div>
                        </section>

                        <!-- â”€â”€â”€ PODSUMOWANIE STOSU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ -->
                        <hr class="section-divider">
                        <div class="section-label">ğŸ“‹ Podsumowanie stosu technologicznego</div>

                        <div class="overflow-x-auto rounded-xl border border-stone-100">
                            <table class="w-full text-xs border-collapse">
                                <thead>
                                    <tr class="bg-stone-900 text-amber-400">
                                        <th class="text-left px-4 py-3 font-black">Etap</th>
                                        <th class="text-left px-4 py-3 font-black">NarzÄ™dzie</th>
                                        <th class="text-left px-4 py-3 font-black">ProtokÃ³Å‚</th>
                                        <th class="text-left px-4 py-3 font-black">Uwagi</th>
                                    </tr>
                                </thead>
                                <tbody class="divide-y divide-stone-100 bg-white">
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Mapa + Lokalizacja</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">slam_toolbox + AMCL</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">/map, /tf</td>
                                        <td class="px-4 py-3 text-stone-500">2D LiDAR lub depthâ†’scan</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Planowanie trasy</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">Nav2</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">NavigateToPose action</td>
                                        <td class="px-4 py-3 text-stone-500">Costmapy 2D, DWB planner</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Detekcja obiektÃ³w</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">YOLOv8 + RealSense</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">/detected_box_position</td>
                                        <td class="px-4 py-3 text-stone-500">Centroid 3D z PointCloud2</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Planowanie chwytu</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">MoveIt 2 + GraspNet</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">MoveGroup API</td>
                                        <td class="px-4 py-3 text-stone-500">Pre-grasp â†’ Grasp â†’ Retreat</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Detekcja czÅ‚owieka</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">YOLOv8-pose + DeepFace</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">/human_info (JSON)</td>
                                        <td class="px-4 py-3 text-stone-500">Klasa 0 (person) + emocje</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Gesty powitania</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">Unitree SDK2 Low-Level</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">rt/lowcmd DDS</td>
                                        <td class="px-4 py-3 text-stone-500">Interpolacja sinusoidalna 200Hz</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Synteza mowy</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">Coqui TTS (XTTS v2)</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">sounddevice (audio out)</td>
                                        <td class="px-4 py-3 text-stone-500">Polski, offline, adaptacja do nastroju</td>
                                    </tr>
                                    <tr class="hover:bg-stone-50">
                                        <td class="px-4 py-3 font-bold">Orkiestracja</td>
                                        <td class="px-4 py-3 font-mono text-blue-600">FSM / BehaviorTree.CPP</td>
                                        <td class="px-4 py-3 font-mono text-stone-500">ROS2 Topics + Actions</td>
                                        <td class="px-4 py-3 text-stone-500">Spinanie wszystkich moduÅ‚Ã³w</td>
                                    </tr>
                                </tbody>
                            </table>
                        </div>

                        <div class="p-7 bg-stone-950 rounded-3xl text-white mt-2">
                            <h3 class="text-xl font-black mb-3 text-amber-400">Uruchomienie peÅ‚nej misji</h3>
                            <div class="code-block" style="margin-bottom: 0; background: #0c0a09;">
                                <pre><code"># Terminal 1: Most ROS2 â†” G1
ros2 launch unitree_ros2 g1_bridge.launch.py interface:=eth0

# Terminal 2: SLAM + Nav2
ros2 launch my_g1_pkg navigation.launch.py map:=~/maps/room_map.yaml

# Terminal 3: Percepcja (YOLO + chmura punktÃ³w)
ros2 run my_g1_pkg box_detector
ros2 run my_g1_pkg human_detector

# Terminal 4: Mowa
ros2 run my_g1_pkg speech_node

# Terminal 5: START MISJI
ros2 run my_g1_pkg mission_fsm</code></pre>
                            </div>
                        </div>

                    </div>
                `
            },
        /* â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
               MODUÅ 9: PROJEKT PRAKTYCZNY â€” Magazynier (Karton & PÃ³Å‚ka)
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
            {
                id: 'logistics_project',
                title: 'Projekt: Alf magazynier',
                icon: 'ğŸ“¦',
                links: [
                    {name: 'Unitree G1 SDK', url: 'https://github.com/unitreerobotics/unitree_sdk2'},
                    {name: 'Nav2 Configuration', url: 'https://navigation.ros.org/configuration/packages/configuring-costmaps.html'},
                    {name: 'YOLOv8 Docs', url: 'https://docs.ultralytics.com/'}
                ],
                description: 'Kompletny poradnik implementacji zadania logistycznego: wykrycie kartonu, zaÅ‚adowanie towaru, transport do wyznaczonej strefy i omijanie przeszkÃ³d.',
                details: `
                    <div class="space-y-6 step-content">

                        <!-- SCENARIUSZ -->
                        <div class="bg-stone-950 rounded-2xl p-7 text-white shadow-xl">
                            <p class="text-amber-500 text-[10px] font-black uppercase tracking-widest mb-3">Scenariusz aplikacji</p>
                            <h3 class="text-2xl font-black mb-4 leading-tight">Zadanie: pakowanie i transport kartonÃ³w</h3>
                            <p class="text-stone-400 text-sm mb-4 leading-relaxed">
                                Robot Unitree G1 EDU ma za zadanie autonomicznie zlokalizowaÄ‡ pusty karton, umieÅ›ciÄ‡ w nim przedmiot (np. butelkÄ™), podnieÅ›Ä‡ karton oburÄ…cz, a nastÄ™pnie zanieÅ›Ä‡ go na oznaczonÄ… pÃ³Å‚kÄ™, dynamicznie omijajÄ…c przeszkody pojawiajÄ…ce siÄ™ na trasie.
                            </p>
                            <div class="flex flex-wrap gap-2">
                                <span class="px-3 py-1 bg-stone-800 border border-stone-700 rounded text-[10px] font-bold uppercase text-amber-500">Dual Arm Manipulation</span>
                                <span class="px-3 py-1 bg-stone-800 border border-stone-700 rounded text-[10px] font-bold uppercase text-blue-400">Dynamic Obstacle Avoidance</span>
                                <span class="px-3 py-1 bg-stone-800 border border-stone-700 rounded text-[10px] font-bold uppercase text-green-400">Marker Detection</span>
                            </div>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 1: PERCEPCJA -->
                        <div class="section-label">ğŸ‘ï¸ Krok 1 â€” Wykrywanie KartonÃ³w i PrzeszkÃ³d</div>

                        <p class="text-stone-600 leading-relaxed mb-3">
                            Wykorzystujemy model <strong>YOLOv8</strong> wytrenowany na klasÄ™ "cardboard_box" oraz kamerÄ™ gÅ‚Ä™bi do precyzyjnego namierzenia Å›rodka kartonu w przestrzeni 3D. Przeszkody sÄ… mapowane automatycznie przez lokalny planer Nav2 (local costmap).
                        </p>

                        <div class="code-block">
                            <pre><code class="language-python">from ultralytics import YOLO
import cv2
import numpy as np

# Inicjalizacja modelu wizyjnego
model = YOLO("yolov8n.pt") # lub wÅ‚asny model: "best_cardboard.pt"

def get_target_position(image, depth_map, target_class="cardboard_box"):
    """
    Zwraca wspÃ³Å‚rzÄ™dne (x, y, dist) najbliÅ¼szego kartonu.
    """
    results = model(image, verbose=False)
    
    for result in results:
        boxes = result.boxes
        for box in boxes:
            cls_id = int(box.cls[0])
            label = model.names[cls_id]
            
            if label == target_class and float(box.conf) > 0.6:
                # Oblicz Å›rodek bounding boxa
                x1, y1, x2, y2 = box.xyxy[0]
                cx, cy = int((x1 + x2) / 2), int((y1 + y2) / 2)
                
                # Pobierz dystans z mapy gÅ‚Ä™bi (w mm -> m)
                # Upewnij siÄ™, Å¼e depth_map jest poprawnym arrayem numpy
                if 0 <= cy < depth_map.shape[0] and 0 <= cx < depth_map.shape[1]:
                    distance = depth_map[cy, cx] / 1000.0
                
                    # JeÅ›li dystans jest prawidÅ‚owy (np. 0.3m - 3.0m)
                    if 0.3 < distance < 3.0:
                        return {"u": cx, "v": cy, "depth": distance}
    return None</code></pre>
                        </div>

                        <div class="pro-tip p-5 rounded-xl">
                            <h4 class="font-bold text-blue-800">Przeszkody a Nav2</h4>
                            <p class="text-sm text-blue-700">
                                Nie musisz pisaÄ‡ wÅ‚asnego kodu do detekcji przeszkÃ³d (np. krzesÅ‚a na drodze). ROS 2 <strong>Nav2 Controller</strong> automatycznie aktualizuje "local_costmap" na podstawie danych z LIDARu/Kamery i wygina Å›cieÅ¼kÄ™ (path planning), aby ominÄ…Ä‡ obiekt w czasie rzeczywistym.
                            </p>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 2: MANIPULACJA -->
                        <div class="section-label">ğŸ¦¾ Krok 2 â€” Manipulacja (Pakowanie i Podnoszenie)</div>

                        <p class="text-stone-600 leading-relaxed mb-3">
                            Podnoszenie kartonu wymaga koordynacji obu ramion (Dual Arm). W G1 EDU uÅ¼ywamy sekwencji: podejÅ›cie -> kucniÄ™cie (obniÅ¼enie CoM) -> Å›ciÅ›niÄ™cie kartonu -> wstanie.
                        </p>

                        <div class="code-block">
                            <pre><code class="language-python">class G1Manipulator:
    def __init__(self, robot_interface):
        self.robot = robot_interface # Wrapper na Unitree SDK2
    
    def fill_and_lift_box(self):
        print("ğŸ¤– Rozpoczynam procedurÄ™ pakowania...")

        # 1. ChwyÄ‡ przedmiot (np. jednÄ… rÄ™kÄ…)
        self.robot.arm_right.move_to(x=0.3, y=-0.2, z=0.1)
        self.robot.gripper_right.close()
        self.robot.arm_right.move_to(x=0.3, y=-0.2, z=0.4) # UnieÅ›

        # 2. UpuÅ›Ä‡ do kartonu (zakÅ‚adamy karton przed robotem)
        self.robot.arm_right.move_to(x=0.4, y=0.0, z=0.3)
        self.robot.gripper_right.open()
        
        # 3. Przygotuj siÄ™ do podniesienia kartonu (Dual Arm)
        # ObniÅ¼ pozycjÄ™ ciaÅ‚a (Squat) dla stabilnoÅ›ci
        self.robot.body.set_height(-0.2) 
        
        # Ustaw rÄ™ce po bokach kartonu
        self.robot.arm_left.move_to(x=0.35, y=0.20, z=0.1)
        self.robot.arm_right.move_to(x=0.35, y=-0.20, z=0.1)
        
        # 4. ÅšciÅ›nij (Force Control)
        # UÅ¼ywamy trybu hybrydowego: pozycja + limit momentu siÅ‚y
        self.robot.arms.squeeze(force_n=30.0)
        
        # 5. WstaÅ„ z obciÄ…Å¼eniem
        self.robot.body.set_height(0.0) # PowrÃ³t do stania
        self.robot.set_gait_mode("load_carrying") # WaÅ¼ne: zmiana trybu chodu!
        return True</code></pre>
                        </div>

                        <hr class="section-divider">

                        <!-- KROK 3: ORKIESTRATOR (FSM) -->
                        <div class="section-label">ğŸ§  Krok 3 â€” Logika GÅ‚Ã³wna (Maszyna StanÃ³w)</div>

                        <p class="text-stone-600 leading-relaxed mb-3">
                            PeÅ‚ny program integrujÄ…cy widzenie, nawigacjÄ™ i manipulacjÄ™ w pÄ™tli decyzyjnej. PamiÄ™taj o inicjalizacji interfejsÃ³w przed uruchomieniem "Worker".
                        </p>

                        <div class="code-block">
                            <pre><code class="language-python">#!/usr/bin/env python3
import time
from enum import Enum

# Stany robota
class State(Enum):
    SEARCH_BOX = 1
    NAVIGATE_TO_BOX = 2
    FILL_AND_LIFT = 3
    NAVIGATE_TO_SHELF = 4
    PLACE_ON_SHELF = 5

class WarehouseWorker:
    def __init__(self, robot_interface):
        self.state = State.SEARCH_BOX
        self.vision = VisionSystem() # Mock vision class
        self.nav = Nav2Client()      # Wrapper na ROS2 Action Client
        # Przekazujemy poprawnie zainicjalizowany interfejs robota
        self.arms = G1Manipulator(robot_interface)
        self.target_shelf_loc = None
        
    def run(self):
        print("ğŸš€ Start misji magazyniera")
        
        while True:
            # Pobranie klatki i gÅ‚Ä™bi (przykÅ‚ad)
            current_obs = self.vision.get_frame()
            depth_map = self.vision.depth
            
            # --- STAN 1: Szukanie kartonu ---
            if self.state == State.SEARCH_BOX:
                target = get_target_position(current_obs, depth_map)
                if target:
                    print(f"ğŸ“¦ Znaleziono karton: {target['depth']:.2f}m")
                    self.nav.set_goal(target['u'], target['depth'])
                    self.state = State.NAVIGATE_TO_BOX
                else:
                    self.nav.spin_slowly() # ObrÃ³t w poszukiwaniu
            
            # --- STAN 2: Dojazd do kartonu ---
            elif self.state == State.NAVIGATE_TO_BOX:
                if self.nav.has_reached_goal():
                    self.nav.stop()
                    self.state = State.FILL_AND_LIFT
                elif self.nav.is_stuck():
                    self.nav.recover()
            
            # --- STAN 3: Manipulacja ---
            elif self.state == State.FILL_AND_LIFT:
                success = self.arms.fill_and_lift_box()
                if success:
                    # Szukaj znacznika pÃ³Å‚ki (np. ArUco ID 42)
                    self.target_shelf_loc = self.vision.find_marker(id=42)
                    if self.target_shelf_loc:
                        self.state = State.NAVIGATE_TO_SHELF
                    else:
                        print("âš ï¸ Nie widzÄ™ pÃ³Å‚ki, obracam siÄ™...")
                        self.nav.spin_slowly()
            
            # --- STAN 4: Transport ---
            elif self.state == State.NAVIGATE_TO_SHELF:
                # Aktualizacja celu w ruchu (Visual Servoing)
                self.nav.update_goal_continuously(self.target_shelf_loc)
                
                if self.nav.distance_to_goal() < 0.5:
                    self.state = State.PLACE_ON_SHELF
            
            # --- STAN 5: OdkÅ‚adanie ---
            elif self.state == State.PLACE_ON_SHELF:
                self.arms.robot.body.set_height(-0.1) # Lekki przysiad
                self.arms.robot.arms.release()        # Zwolnij Å›cisk
                self.arms.robot.body.move_backwards(0.5) 
                print("âœ… Zadanie zakoÅ„czone sukcesem!")
                break
                
            time.sleep(0.1)

if __name__ == "__main__":
    # Mock klasy interfejsu dla celÃ³w demonstracyjnych
    # W produkcji uÅ¼yj: from unitree_sdk2.python import RobotInterface
    class RobotInterfaceMock: 
        pass 
    
    robot = RobotInterfaceMock() 
    # Tutaj inicjalizujemy Workera z instancjÄ… robota
    worker = WarehouseWorker(robot)
    worker.run()</code></pre>
                        </div>

                        <div class="critical-warn p-5 rounded-xl">
                            <h4 class="font-bold text-red-800">Uwaga: StabilnoÅ›Ä‡ podczas chodu</h4>
                            <p class="text-sm text-red-700">
                                Gdy robot niesie karton, Å›rodek ciÄ™Å¼koÅ›ci (CoM) przesuwa siÄ™ do przodu o okoÅ‚o 10-15 cm. NaleÅ¼y skompensowaÄ‡ to w Unitree SDK, ustawiajÄ…c <code>body_pitch</code> na lekko dodatniÄ… wartoÅ›Ä‡ (odchylenie tuÅ‚owia do tyÅ‚u) lub uÅ¼ywajÄ…c dedykowanego trybu "HighCmd.gaitType = 2" (Load Carrying), jeÅ›li firmware na to pozwala.
                            </p>
                        </div>

                    </div>
                `
            }
        ];

        let currentIdx = 0;
        const state = { completed: new Set() };

        function renderNav() {
            const list = document.getElementById('nav-list');
            list.innerHTML = '';
            modules.forEach((mod, i) => {
                const active = i === currentIdx;
                const done = state.completed.has(i);

                const item = document.createElement('div');
                item.className = `p-3 rounded-xl cursor-pointer transition-all flex items-center gap-3 ${active ? 'nav-item-active' : 'hover:bg-stone-900 text-stone-500'}`;
                item.onclick = () => { currentIdx = i; renderApp(); };

                item.innerHTML = `
                    <span class="text-xl">${done ? 'âœ…' : mod.icon}</span>
                    <span class="text-sm font-bold tracking-tight">${mod.title}</span>
                `;
                list.appendChild(item);
            });
        }

        function renderContent() {
            const mount = document.getElementById('content-mount');
            const mod = modules[currentIdx];

            const linksHtml = mod.links.map(l => `
                <a href="${l.url}" target="_blank" class="text-[10px] font-bold text-amber-600 border border-amber-200 px-3 py-1 rounded-full hover:bg-amber-50">
                    ${l.name} â†—
                </a>
            `).join('');

            mount.innerHTML = `
                <div class="mb-10">
                    <span class="text-xs font-black text-amber-500 uppercase tracking-widest">Krok ${currentIdx + 1}</span>
                    <h2 class="text-4xl font-extrabold text-stone-900 mb-4">${mod.title}</h2>
                    <p class="text-lg text-stone-500 mb-6">${mod.description}</p>
                    <div class="flex flex-wrap gap-2">${linksHtml}</div>
                </div>
                <div class="prose max-w-none">${mod.details}</div>
            `;

            document.getElementById('main-scroll-area').scrollTop = 0;
            document.getElementById('prev-btn').disabled = currentIdx === 0;
            document.getElementById('next-btn').innerText = currentIdx === modules.length - 1 ? 'ZakoÅ„cz âœ“' : 'NastÄ™pny Krok â†’';
            document.getElementById('footer-label').innerText = `ModuÅ‚: ${mod.title}`;
        }

        function changeModule(dir) {
            if (dir === 1) state.completed.add(currentIdx);
            const next = currentIdx + dir;
            if (next >= 0 && next < modules.length) {
                currentIdx = next;
                renderApp();
            }
        }

        function updateProgress() {
            const p = Math.round((state.completed.size / modules.length) * 100);
            document.getElementById('top-progress').style.width = `${p}%`;
            document.getElementById('inner-progress').style.width = `${p}%`;
            document.getElementById('progress-val').innerText = `${p}%`;
        }

        function toggleSidebar() {
            document.getElementById('sidebar').classList.toggle('-translate-x-full');
        }

        function renderApp() {
            renderNav();
            renderContent();
            updateProgress();
        }

        window.onload = renderApp;
    </script>
</body>
</html>
